# Обучение с подкреплением: награды и действия

Обучение с подкреплением (reinforcement learning, RL) — это один из самых интересных и сложных подходов в машинном обучении. В отличие от обучения с учителем, где модель получает готовые ответы, здесь агент учится на основе **награды** (reward), которую он получает за свои действия. В этой статье мы разберём, как работает RL, на примере виртуальной "машинки", которая учится ездить по трассе.

---

## Что такое reward?

**Reward** — это ключевой элемент обучения с подкреплением. Это численная оценка, которая показывает, насколько хорошо агент справился с задачей. Простой способ — использовать значения `+1` и `-1`:

- `+1` — агент успешно завершил задачу (например, проехал трассу без аварий).
- `-1` — агент провалился (врезался в стену).

В начале обучения агент совершает много ошибок. Например, он может врезаться в стену, совершая 197 действий за 19.7 секунд (10 действий в секунду). В конце эпизода — **минус 1**. Если же он проехал всю трассу — **плюс 1**.

---

## Как агент принимает решения?

Агент последовательно выполняет действия, которые влияют на его путь. Каждое действие — это выбор, например:

- Повернуть руль на 7 градусов влево и ускориться на 2%.
- Оставить руль на месте и затормозить на 3%.

Эти действия формируют **цепочку решений**, и в конце агент получает reward. Но как понять, какие действия были ключевыми для успеха или провала?

---

## Пробрасывание reward в прошлое

Вот здесь и начинается сложная часть: **надо определить, как reward влияет на предыдущие действия**. Это называется **пробрасыванием reward в прошлое**.

Например, если агент врезался, мы не можем сразу сказать, что виновато последнее действие. Возможно, ошибка была раньше — в момент, когда он слишком резко вошёл в поворот.

Вот как это можно решить:

- Назначить **минус 1** всем действиям, которые происходили за секунду до аварии.
- Назначить **минус 0.1** всем действиям, которые происходили более чем за 5 секунд до аварии.
- Или вообще не учитывать последние действия, если они уже не могли повлиять на результат.

Это позволяет превратить задачу в **обучение с учителем**, где каждому эпизоду присвоена оценка.

---

## Стратегии назначения reward

Существует множество подходов к назначению reward. Вот основные:

- **Постепенное уменьшение reward**: каждое предыдущее действие получает reward, умноженный на 0.99. Так мы "размазываем" ответственность по всему эпизоду.
- **Ключевые моменты**: только последние несколько действий получают reward. Например, последние 3 действия — минус 1, остальные — 0.
- **Важность первого действия**: первое действие считается самым важным, а последующие — менее значимыми. Это может быть полезно в задачах, где начальный выбор определяет всё.

---

## Промежуточные reward

Reward не обязательно должен быть только в конце. Можно назначать его **на каждом шаге**, чтобы агент учился быстрее. Например:

- За каждый процент пройденной трассы — `+0.001`.
- За приближение к краю — `-0.01`.
- За высокую скорость — `+0.005`.

Это помогает агенту понять, что "хорошо" и "плохо" не только в конце, но и в процессе. Например, если машина проехала 57% трассы, а потом врезалась, её reward будет `+0.57 - 1 = -0.43`. Это лучше, чем просто `-1`, потому что агент видит, что он "почти победил".

---

## Использование человеческих данных

Если агент не может быстро обучиться только на основе reward, можно использовать **данные от людей**. Например, симулировать, как человек управляет машиной, и использовать эти данные как стартовую точку для обучения.

Это помогает агенту быстрее находить "хорошие" решения, потому что он уже имеет примеры, где reward `+1`.

---

## Где применяется обучение с подкреплением?

Обучение с подкреплением применяется в самых разных областях:

- **Автопилоты** — машины учатся ездить по трассе.
- **Боты для игр** — например, в Dota 2 или StarCraft.
- **GPT и NLP** — модели учатся на основе обратной связи от пользователей.
- **Робототехника** — обучение ходить, бегать, лазать.
- **Финансы** — трейдинг, управление портфелем.
- **Нейропродажники** — агенты, которые учатся продавать, задавать вопросы, закрывать возражения.

---

## Заключение

Обучение с подкреплением — это мощный инструмент, но он требует тщательной настройки reward. Какой способ назначения reward выбрать — зависит от задачи. Можно использовать готовые библиотеки, но если вы хотите поймать суть, попробуйте реализовать всё вручную.

**Важно помнить:** reward — это не только "плюс" или "минус", это **оценка значимости действий**. И как раз она помогает агенту учиться, несмотря на отсутствие "правильных ответов" на каждом шаге.