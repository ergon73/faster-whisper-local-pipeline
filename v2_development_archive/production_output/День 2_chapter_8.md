# Маленькие трансформеры и их возможности

В последнее время всё чаще встаёт вопрос: а нужна ли нам гигантская модель вроде `Llama` или `GPT`, если можно обойтись чем-то более лёгким? В этой статье мы разберём, как работают **маленькие трансформеры**, какие задачи они решают и как их применять на практике.

---

## Почему маленькие трансформеры?

### Основная идея

Маленькие трансформеры — это модели, которые значительно меньше по размеру, чем их "боссы" вроде `GPT-4` или `Llama`. Например, упомянутая в докладе модель имеет всего **около 80 тысяч параметров**, в то время как `Llama` — **80 миллиардов**. То есть разница в **миллион раз**. 

Это не метафора — в прямом смысле, в миллион раз меньше.

### Преимущества

- **Быстродействие**: такие модели работают **супер быстро**, не требуя мощных видеокарт.
- **Низкие требования**: можно запускать даже на **процессоре**, без GPU.
- **Мобильность**: такие модели могут работать **локально**, даже на мобильных устройствах.
- **Гибкость**: их можно использовать как часть более сложных систем — например, в связке с `Llama` или `GPT`, чтобы сначала обработать запрос, а потом передать его на более глубокую обработку.

---

## Примеры использования

Маленькие трансформеры отлично подходят для:

- **Быстрого поиска ответов**.
- **Подбора материалов**.
- **Интеграции в чат-боты и веб-сервисы**.
- **Работы в колл-центрах** (например, автоматизация простых диалогов).

Они не смогут писать стихи в стиле Пушкина или решать сложные логические задачи, но для многих повседневных задач — **в самый раз**.

---

## Как создать и использовать маленький трансформер?

### Шаг 1: Установка библиотек

Для работы с трансформерами на Python можно использовать библиотеку `Transformers` от Hugging Face. Вот как установить её: