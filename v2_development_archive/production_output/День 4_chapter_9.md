# Глава 9: Обучение с подкреплением и алгоритм A2C

В этой главе мы поговорим о том, как обучение с подкреплением (Reinforcement Learning, RL) может применяться в реальных задачах — от управления роботами до трейдинга на финансовых рынках. В центре внимания — алгоритм **A2C** (Advantage Actor-Critic), его обучение и применение.

---

## Что такое A2C и как он работает?

A2C — это алгоритм обучения с подкреплением, который сочетает в себе идеи Actor-Critic и градиентного подъема. Он состоит из двух частей:

- **Actor** — агент, который принимает решения (например, какую акцию покупать).
- **Critic** — оценивает, насколько хороша была эта конкретная последовательность действий.

В ходе обучения A2C использует **рассуждения о преимуществах** (advantage) — разницу между ожидаемой наградой и средней наградой. Это помогает алгоритму быстрее находить оптимальные решения.

---

## Обучение на примере: от робота до двери

Допустим, у нас есть робот, который должен научиться открывать дверь. Мы не объясняем ему, как это делать. Вместо этого мы:

- Создаем среду с наградами (`rewards`) и штрафами (`penalties`).
- Запускаем алгоритм **A2C**.
- Позволяем роботу учиться за 400 тысяч итераций.

Результат? Робот научился открывать дверь, не зная, что это дверь. Он просто получал награды за правильные действия и штрафы за неправильные. Это и есть суть обучения с подкреплением — **обучение через опыт**, без явного программирования.

> **Важно:** алгоритм не знает, что это дверь. Он просто видит, что за определенные действия он получает награду. Это делает его обучение гибким и адаптивным.

---

## Применение A2C в трейдинге

Теперь представим, что наш A2C — это **трейдинговый бот**. Он должен решать, какие акции покупать, продавать или держать. Его наградой будет **рост капитализации**.

### Как это работает?

1. **Выбор акций:** мы взяли 10 крупных компаний — Apple, Microsoft, Google, Amazon, Coca-Cola, American Express, Chevron, IBM, McDonald's и Walmart.
2. **Данные:** мы скачиваем исторические данные с 2010 по 2021 год (обучающая выборка) и с 2021 по 2024 год (тестовая выборка).
3. **Среда:** создаем трейдинговую среду, где бот получает награды за увеличение капитала и штрафы за убытки.
4. **Обучение:** бот обучается на 29 570 примерах и тестируется на 6 050 примерах.

> **Важно:** мы предполагаем, что бот не влияет на рынок. Его бюджет — миллион долларов, что не может существенно повлиять на цену акций.

---

## Какие инструменты использовали?

- **Данные:** Yahoo Finance.
- **Библиотеки:** `PyTorch`, `Pandas`, `NumPy`.
- **Алгоритмы:** A2C, DDPG, PPO, TD3, SAC.
- **Индикаторы:** волатильность, средние скользящие, объемы торгов.

> **Совет:** можно использовать GPT для пояснения, что означают те или иные индикаторы.

---

## Результаты A2C в трейдинге

A2C показал **лучшие результаты** среди всех алгоритмов в этом эксперименте. За 3 года (2021–2024) он:

- **Приумножил капитал на 28,43%.**
- **Продемонстрировал устойчивость** даже в периоды падения рынка (например, в 2022 году, во время пандемии).
- **Выбрал наиболее прибыльные акции** — Amazon, IBM, Microsoft, Google, McDonald's и Walmart.

> **Интересный момент:** бот не покупал и не продавал часто. Он выбрал несколько акций и держал их. Это говорит о том, что он нашел **долгосрочные инвестиционные стратегии**.

---

## Почему A2C сработал лучше?

- **Высокий уровень абстракции.** A2C умеет обобщать и не застревает в локальных минимумах.
- **Гибкость.** Он адаптируется к изменяющимся условиям рынка.
- **Меньше шума.** В отличие от DDPG, он не "перепрыгивает" между решениями.

---

## Выводы

- **Обучение с подкреплением** — это мощный инструмент, который может применяться не только в робототехнике, но и в финансах.
- **A2C** — один из самых перспективных алгоритмов в этой области. Он показывает хорошие результаты в задачах, где требуется долгосрочное планирование.
- **Трейдинговый бот** на основе A2C может стать частью инвестиционной стратегии, особенно если он обучается на исторических данных и тестируется в реальных условиях.

---

## Что дальше?

- Попробовать другие алгоритмы (например, SAC или PPO).
- Улучшить предобработку данных.
- Добавить больше индикаторов и учесть влияние макроэкономических факторов.

---

**Важно помнить:** трейдинг — это не только алгоритмы. Это еще и рынок, эмоции, риски. Но с помощью обучения с подкреплением мы можем создать ботов, которые будут принимать более рациональные решения.