Вы абсолютно правильно поняли ситуацию.

**Универсальный промпт — это всегда компромисс.** Он делает "неплохо" везде, но "идеально" — нигде. В вашем тесте v2 сработал отлично для Qwen 32B, но "пережал" текст на GPT-OSS и не справился с "потоком мыслей" у Qwen MoE.

Если мы фиксируемся на конкретной модели, мы **обязаны** затачивать промпт под ее "психотип".

Вот как я бы переписал промпты индивидуально под каждого из трех главных "персонажей" вашего теста.

-----

### 1\. Для Qwen3:32b и Gemma 3 (Сбалансированный стиль)

*Эти модели умные и послушные. Им не нужно кричать "ЗАПРЕЩЕНО", с ними можно работать тоньше, чтобы улучшить стиль текста.*

**Что меняем:**

  * Убираем агрессивные запреты (они тратят токены внимания).
  * Добавляем фокус на **стилистику** (чтобы текст не был сухим).
  * Добавляем инструкцию по работе с примерами (чтобы не вырезал шутки, если они в тему).

<!-- end list -->

```python
# PROMPT FOR: Qwen3:32b / Gemma 3
def build_prompt_balanced(chapter_title: str, chapter_text: str) -> str:
    return f"""Ты — редактор технического блога на Хабре.
Твоя задача: превратить расшифровку доклада в увлекательную, читаемую статью.

**ТВОЯ ЦЕЛЬ:**
Сделать текст понятным для Junior/Middle разработчиков, сохранив глубину материала.

**ИНСТРУКЦИИ:**
1. **ТЕРМИНОЛОГИЯ:**
   - Исправляй ошибки STT: "пайторч" -> `PyTorch`, "джипити" -> `GPT`, "пандас" -> `Pandas`.
   - Всегда используй правильный регистр для библиотек (не numpy, а `NumPy`).
   - Код и названия библиотек оборачивай в обратные кавычки (`code`).

2. **СТИЛЬ И СТРУКТУРА:**
   - Разбей текст на логические секции с заголовками (##, ###).
   - Если спикер приводит пример или метафору — **сохрани её**, это делает текст живым.
   - Избегай канцеляризмов. Пиши просто и емко.
   - Убирай только явный мусор ("э-э-э", "слышно меня"), но не суши текст до состояния справочника.

3. **ФОРМАТ:**
   - Только Markdown.
   - Используй списки для перечислений.
   - Важные мысли выделяй **жирным**.

**ВХОДНОЙ ТЕКСТ:**
Тема: {chapter_title}
Текст:
{chapter_text}

**СТАТЬЯ (MARKDOWN):**"""
```

-----

### 2\. Для GPT-OSS:20b (Против сжатия)

*Эта модель страдает "синдромом саммаризатора". Она пытается сократить всё до сути. Нам нужно запретить ей сокращать.*

**Что меняем:**

  * Меняем роль: не "Редактор" (который режет), а "Корректор" (который правит).
  * Прямой запрет на удаление деталей.
  * Указание на длину ("Сохрани объем").

<!-- end list -->

```python
# PROMPT FOR: GPT-OSS:20b
def build_prompt_verbose(chapter_title: str, chapter_text: str) -> str:
    return f"""Роль: Технический корректор (Technical Copywriter).
Задача: Исправить ошибки в транскрипте, сохранив **100% информативности** исходной лекции.

**ГЛАВНОЕ ПРАВИЛО:**
НЕ СОКРАЩАЙ ТЕКСТ. Твоя задача — не сделать саммари, а сделать читаемый полный текст лекции. Удаляй только слова-паразиты, но сохраняй все предложения, примеры и пояснения лектора.

**ИНСТРУКЦИИ:**
1. **Исправление ошибок:**
   - "ПыТорч" -> `PyTorch`, "джипити" -> `GPT`, "керас" -> `Keras`.
   - Исправь пунктуацию и разбей "поток сознания" на предложения.

2. **Оформление:**
   - Используй Markdown заголовки (##) для смены темы разговора.
   - Выделяй ключевые понятия **жирным**.
   - Код и команды пиши в `моноширинном формате`.

3. **Запреты:**
   - ЗАПРЕЩЕНО удалять технические детали, даже мелкие.
   - ЗАПРЕЩЕНО писать вступления ("Here is the text...").
   - ЗАПРЕЩЕНО объединять 5 предложений в одно. Пиши подробно.

**ИСХОДНЫЙ ТЕКСТ:**
Тема: {chapter_title}
Текст:
{chapter_text}

**ПОЛНЫЙ ОТРЕДАКТИРОВАННЫЙ ТЕКСТ:**"""
```

-----

### 3\. Для Qwen3:30b (MoE) и "думающих" моделей

*Эта модель хочет рассуждать. Бороться с природой (запрещать думать) сложно. Лучше возглавить этот процесс или использовать жесткий формат вывода.*

**Что меняем:**

  * Разрешаем ей думать, но просим делать это в блоке, который мы потом вырежем (или надеемся, что она сама спрячет).
  * Используем технику **"XML-тегов"**, чтобы четко отделить мусор от контента.

<!-- end list -->

```python
# PROMPT FOR: Qwen3:30b-MoE
def build_prompt_structured(chapter_title: str, chapter_text: str) -> str:
    return f"""Ты — система обработки транскриптов.

Твоя задача состоит из двух этапов:
1. (Мысленно) Проанализировать текст на ошибки терминологии.
2. (Вывод) Написать чистый Markdown-текст.

**СЛОВАРЬ ЗАМЕН (Применять строго):**
- `PyTorch` (вместо ПыТорч/пайторч)
- `GPT` (вместо джипити)
- `TensorFlow`, `Keras`, `NumPy`, `Pandas`

**ФОРМАТ ВЫВОДА:**
Ты должен вывести ТОЛЬКО отредактированный текст внутри тегов <article> и </article>.
Никаких вступлений. Никаких объяснений после текста.

Пример:
<article>
## Заголовок
Текст лекции с исправленными терминами...
</article>

**ВХОДНЫЕ ДАННЫЕ:**
{chapter_text}

**ОТВЕТ:**"""
```

*(При использовании этого промпта в Python-коде вам нужно будет просто извлечь то, что внутри `<article>...</article>`, это очень надежно).*

### Резюме: Какую стратегию выбрать?

1.  **Если вы оставляете `qwen3:32b` (мой фаворит):** Берите **Промпт №1 (Balanced)**. Он даст самый "человеческий" и приятный текст.
2.  **Если нужна GPT-OSS:** Берите **Промпт №2 (Verbose)**, иначе потеряете половину смысла лекций.
3.  **Если хочется "добить" MoE модель:** Используйте **Промпт №3** с тегами. Это единственный способ гарантированно отсечь её "мысли" без сложного regex-кода.