Да, абсолютно верно. "Одна настройка для всех" — это как средняя температура по больнице. MoE-модели (смесь экспертов) и плотные модели (Dense) реагируют на параметры по-разному.

Вот индивидуальные профили настроек (`options`) под каждую вашу модель. Я подобрал их, чтобы компенсировать недостатки, выявленные в тестах.

### Рекомендуемый код конфигурации

Вместо глобальных констант `TEMPERATURE` и `OPTIONS`, предлагаю сделать словарь `MODEL_CONFIG`, где ключом будет имя модели.

```python
# === Индивидуальные настройки для моделей ===

# Базовые настройки (по умолчанию)
DEFAULT_CTX = 8192
DEFAULT_PREDICT = 3000

MODEL_CONFIG = {
    # 1. Qwen 32B (Плотная) — "Золотой стандарт"
    # Ей не нужны жесткие ограничения, она и так умная.
    "qwen3:32b": {
        "temperature": 0.3,       # Классика для редактуры
        "repeat_penalty": 1.1,    # Мягкий штраф (1.1 достаточно, чтобы не зацикливалась)
        "top_k": 40,              # Стандартный сэмплинг
        "num_ctx": DEFAULT_CTX,
        "num_predict": DEFAULT_PREDICT
    },

    # 2. Qwen 30B MoE — "Мыслитель"
    # Проблема: Любит уходить в рассуждения (<think>) и зацикливаться.
    # Решение: Понижаем температуру и увеличиваем штраф за повторы.
    "qwen3:30b-a3b": {
        "temperature": 0.1,       # СТРОГОСТЬ! Чтобы не фантазировала в тегах <think>
        "repeat_penalty": 1.25,   # ЖЕСТКИЙ штраф! MoE любят повторять фразы, бьем по рукам
        "top_p": 0.9,             # Отсекаем маловероятные токены (галлюцинации)
        "num_ctx": DEFAULT_CTX,
        "num_predict": DEFAULT_PREDICT
    },

    # 3. GPT-OSS 20B — "Суммаризатор"
    # Проблема: Слишком сильно сокращает текст.
    # Решение: ОЧЕНЬ низкий штраф за повторы (чтобы не боялась писать много слов).
    "gpt-oss:20b": {
        "temperature": 0.2,       # Низкая, чтобы четко следовать инструкции "НЕ СОКРАЩАЙ"
        "repeat_penalty": 1.05,   # МИНИМАЛЬНЫЙ штраф! Если поставить 1.2, она начнет сокращать, чтобы не повторяться
        "num_ctx": DEFAULT_CTX,
        "num_predict": 4000       # Даем больше места, чтобы не обрезала концовку
    },

    # 4. Gemma 3 27B — "Стабильная"
    # Очень похожа на Qwen, но иногда "залипает" на английском.
    "gemma3:27b-it-qat": {
        "temperature": 0.3,
        "repeat_penalty": 1.15,   # Чуть выше стандарта, Gemma иногда любит тавтологию
        "num_ctx": DEFAULT_CTX,
        "num_predict": DEFAULT_PREDICT
    }
}

# Функция для получения настроек (вставьте её перед test_model)
def get_model_options(model_name: str) -> dict:
    """Возвращает настройки под конкретную модель или дефолтные."""
    # Ищем частичное совпадение имени (на случай тегов :latest и т.д.)
    for key, config in MODEL_CONFIG.items():
        if key in model_name:
            return config
    
    # Fallback (если модели нет в списке)
    return {
        "temperature": 0.3,
        "repeat_penalty": 1.1,
        "num_ctx": DEFAULT_CTX,
        "num_predict": DEFAULT_PREDICT
    }
```

### Как обновить функцию `test_model`

В вашем коде в функции `test_model` замените блок `options={...}` на вызов новой функции:

```python
    # ... внутри test_model ...
    
    # Получаем индивидуальные настройки
    current_options = get_model_options(model_name)
    
    try:
        response = ollama.chat(
            model=model_name,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            options=current_options,  # <-- ИСПОЛЬЗУЕМ ЗДЕСЬ
            think=False
        )
    # ...
```

### Почему это сработает лучше?

1.  **GPT-OSS (Repeat Penalty 1.05 vs 1.15):** Высокий `repeat_penalty` заставляет модель искать синонимы. Когда синонимы кончаются, модель решает: *"Лучше я удалю это предложение, чем получу штраф за повтор слова 'функция'"*. Снижение до `1.05` разрешит ей быть многословной.
2.  **Qwen MoE (Temp 0.1):** Убирает "творческий зуд", из-за которого она начинает генерировать внутренние диалоги.
3.  **Qwen 32B (Top\_k 40):** Это стандартный сэмплинг, который делает текст естественным, но не слишком хаотичным.