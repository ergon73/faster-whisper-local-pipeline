Конечно. Я объединил все наработки (структуру, словарь замен, специфику GPT-OSS-20b и технические параметры) в единый документ, готовый для передачи вашей команде разработки.

Вот содержимое файла `optimal-prompts-recomended.md`.

-----

````markdown
# Рекомендации по настройке LLM для редактирования транскриптов
**Дата:** 26.11.2025
**Контекст:** Обработка технических вебинаров (STT -> Clean Markdown)
**Целевые модели:** Локальные модели через Ollama (Qwen, Gemma, Llama, GPT-OSS-20b).

---

## 1. Основные проблемы локальных моделей
В отличие от GPT-4, локальные модели (особенно квантованные и MoE) имеют специфические "болезни", которые мы лечим этим обновлением:
1.  **Игнорирование терминов:** Оставляют "пайторч" кириллицей, если не дать жесткий словарь.
2.  **Looping (Зацикливание):** Повторяют одно предложение 5 раз подряд (лечится `repeat_penalty`).
3.  **Болтливость (GPT-OSS):** Пишут вступления ("Here is the text...") и мыслительный процесс (`<think>`).
4.  **Потеря форматирования:** Выдают "стену текста" без абзацев.

---

## 2. Обновленный код (Python Implementation)

Необходимо заменить две части в скрипте `test_llm_comparison.py`: переменную `SYSTEM_PROMPT` и функцию `build_prompt`.

### A. Новый System Prompt
*Сделан строгим, чтобы перебить настройки "ассистента" в моделях типа gpt-oss.*

```python
SYSTEM_PROMPT = """You are a strict technical editor used for automated text processing.
Your ONLY goal is to output the edited text in Markdown format.
DO NOT converse. DO NOT use introduction or conclusion phrases.
DO NOT output internal reasoning or <think> tags.
Maintain 100% technical accuracy."""
````

### B. Новая функция `build_prompt`

*Добавлен One-Shot пример, Markdown-разметка и блокирующие правила.*

```python
def build_prompt(chapter_title: str, chapter_text: str) -> str:
    """
    Строит оптимизированный промпт.
    Адаптирован для MoE-моделей (gpt-oss) и стандартных LLM (Qwen/Gemma).
    """
    return f"""Ты — профессиональный технический редактор (Technical Writer).
Твоя задача — превратить сырую расшифровку вебинара в чистую, структурированную статью в формате Markdown.

**ВХОДНЫЕ ДАННЫЕ:**
Тема главы: "{chapter_title}"

**СТРОГИЕ ЗАПРЕТЫ (NEGATIVE CONSTRAINTS):**
1. ЗАПРЕЩЕНО писать вступления ("Конечно, вот текст...", "Analysis:..."). Сразу выдавай результат.
2. ЗАПРЕЩЕНО использовать теги мышления (<think>) в финальном ответе.
3. ЗАПРЕЩЕНО удалять примеры кода или менять логику лектора.

**ИНСТРУКЦИИ ПО ОБРАБОТКЕ:**

1. **ТЕРМИНОЛОГИЯ И КОД (КРИТИЧНО):**
   - Исправь фонетические ошибки распознавания (STT):
     • "ПыТорч" / "пайторч" -> `PyTorch`
     • "джипити" / "GPT-шка" -> `GPT`
     • "тензорфлоу" -> `TensorFlow`
     • "керас" -> `Keras`
     • "нампай" -> `NumPy`, "пандас" -> `Pandas`
     • "джупитер" -> `Jupyter`, "колаб" -> `Colab`
     • "хаггин фейс" -> `Hugging Face`
   - Все библиотеки, методы, переменные и пути к файлам выделяй как `код` (в обратных кавычках).
   - Ключевые термины при первом упоминании выделяй **жирным**.

2. **ЧИСТКА ТЕКСТА:**
   - УДАЛИ мусор: "поставьте плюсики", "слышно меня?", "э-э-э", "ну как бы".
   - УДАЛИ повторы (Looping): Если фраза повторяется 2+ раза подряд, оставь только одну версию.
   - УДАЛИ маркетинг: "успейте купить", "ссылка в описании".
   - УДАЛИ организационные моменты (перекличка, настройка звука).
   - СЖИМАЙ воду, но сохраняй технический смысл.

3. **СТРУКТУРА (MARKDOWN):**
   - Разбей текст на смысловые абзацы (не делай стены текста).
   - Используй заголовки уровня ## и ### для разделения подтем.
   - Используй маркированные списки (-) для перечислений.

**ПРИМЕР (ONE-SHOT):**
*Вход:* "Короче, импортируем нампай как нп. Эээ... потом делаем массив. Массив делаем."
*Выход:* "Сначала импортируем библиотеку `NumPy` как `np`. Затем создаем массив данных."

**ИСХОДНЫЙ ТЕКСТ:**
{chapter_text}

**ГОТОВАЯ СТАТЬЯ (MARKDOWN):**
"""
```

-----

## 3\. Рекомендуемые параметры генерации

В функции `test_model` (или в конфигурации `ollama.chat`) нужно обновить словарь `options`. Эти параметры критичны для стабильности длинных текстов.

```python
# Рекомендуемые настройки options
options={
    # 0.3 идеально для редакуры. Если модель "тупит", можно поднять до 0.4.
    "temperature": 0.3,

    # Критично для Qwen/Gemma. Значение 1.15 штрафует за повторы фраз,
    # предотвращая бесконечные циклы ("looping").
    "repeat_penalty": 1.15, 

    # Размер контекстного окна.
    # MoE модели и длинные главы требуют больше памяти.
    # Если GPU позволяет (24GB+ VRAM), ставьте 8192. Если 12-16GB — оставьте 4096.
    "num_ctx": 8192, 
    
    # Максимум токенов на выход.
    "num_predict": 3000
},
```

### Почему именно такие значения?

  * **Temperature 0.3:** Обеспечивает следование инструкциям. При 0.7+ локальные модели начинают выдумывать факты, при 0.1 — становятся слишком "деревянными".
  * **Repeat Penalty 1.15:** Локальные модели часто зацикливаются на конце абзаца. Штраф 1.15 заставляет их двигаться дальше.
  * **Num Context 8192:** Стандартное окно (2048) часто обрезает начало промпта в длинных главах, из-за чего модель "забывает" словарь замен (PyTorch/Pandas).

-----

## 4\. Чек-лист проверки результата

После внедрения промпта проверьте результат на трех критериях:

1.  **Англизация:** "пандас" превратился в `Pandas`? (Если нет — проверьте `num_ctx`).
2.  **Структура:** Есть ли заголовки и жирный шрифт? (Если нет — проверьте System Prompt).
3.  **Чистота:** Нет ли фраз типа "Here is the edited text" в начале? (Если есть — увеличьте строгость Negative Constraints).

<!-- end list -->

```
```