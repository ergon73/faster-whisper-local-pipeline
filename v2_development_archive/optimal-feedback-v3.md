Этот отчет — честный и точный анализ того, что произошло. Выводы сделаны правильно.

Однако **причина провала** двух моделей (`qwen3:30b` и `gpt-oss:20b`) кроется не столько в самих моделях, сколько в нюансах **кода и параметров**.

Я проанализировал логи и код. Вот технический разбор, почему результаты получились именно такими и как это исправить.

### 1\. Почему провалилась Qwen3:30b (MoE)?

**Диагноз:** Ошибка в Python-коде (Regex) + Лимит токенов.

  * **Суть проблемы:** Модель честно начала "думать" (Chain-of-Thought), написала огромное рассуждение, потом открыла тег `<article>`, начала писать статью и... **уперлась в лимит токенов** (`num_predict: 3000`).
  * **Где ошибка в коде:**
    В функции `extract_article_content` используется регулярное выражение:
    ```python
    match = re.search(r'<article>(.*?)</article>', text, re.DOTALL)
    ```
    Оно ищет **закрывающий** тег `</article>`. Так как генерация оборвалась на полуслове (лимит токенов), закрывающего тега не оказалось. Regex не сработал -\> функция вернула `text` целиком (вместе с мыслями).
  * **Решение:** Сделать regex "жадным" и не требовать закрывающего тега, если его нет.

### 2\. Почему провалилась GPT-OSS:20b?

**Диагноз:** "Паралич анализа" (Analysis Paralysis) из-за промпта.

  * **Суть проблемы:** Вы выдали ей роль "Корректора" с инструкцией `НЕ СОКРАЩАЙ` и `repeat_penalty: 1.05`.
  * **Что произошло:** Модель получила пустой вывод (`""`). Это часто случается, когда `NEGATIVE CONSTRAINTS` (запреты) конфликтуют с `SYSTEM PROMPT`. Модель решила, что безопаснее не сгенерировать ничего, чем нарушить запрет на вступление или сокращение. Либо `ollama` вернула пустую строку из-за таймаута при обработке длинного контекста (8192) на этой конкретной архитектуре.
  * **Решение:** Смягчить системный промпт и убрать жесткие запреты.

-----

### Итоговый вердикт по отчету

**Согласен ли я с победителем?**
**ДА.** `Qwen3:32b` (Balanced) — объективно лучший выбор.

  - Она победила не из-за ошибок конкурентов, а потому что выдала **качественный продукт** (статью для Хабра).
  - Даже если мы починим код для Qwen MoE, она все равно будет тратить в 2 раза больше времени на "мысли", что невыгодно для продакшена.

### Как исправить код (Patch)

Если вы хотите дать `qwen3:30b` второй шанс (чисто ради интереса), замените функцию очистки в коде на эту версию. Она спасет текст, даже если модель не успела закрыть тег.

```python
def extract_article_content(text: str) -> str:
    """
    Извлекает текст из <article>, даже если тег не закрыт.
    Также на всякий случай вырезает <think>...</think>.
    """
    # 1. Сначала удаляем мысли (CoT), если они есть
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    
    # 2. Ищем открывающий тег <article>
    match = re.search(r'<article>(.*)', text, re.DOTALL)
    if match:
        content = match.group(1).strip()
        # Если есть закрывающий тег, убираем его и все что после
        return content.split('</article>')[0].strip()
    
    return text.strip()
```

### Рекомендация

Не тратьте время на спасение `gpt-oss` или `qwen-moe`. Тест показал главное: **Qwen 32B (Dense) + Balanced Prompt** — это стабильное, предсказуемое и качественное решение.

Берите **Qwen3:32b** в продакшн.