# Обучение с подкреплением: настройка среды и моделей

## Введение

Обучение с подкреплением (reinforcement learning, RL) — это подход в машинном обучении, при котором агент учится принимать решения, взаимодействуя с окружающей средой и получая награды или штрафы за свои действия. В этой статье мы рассмотрим, как настроить среду и обучить агента с помощью популярных библиотек и алгоритмов.

Мы будем использовать готовую среду `HighwayFast-v0` и три популярных алгоритма: `DQN`, `PPO` и `A2C`. В ходе обучения будем отслеживать ключевые метрики и сравнивать эффективность алгоритмов.

---

## Подготовка среды

Готовые среды для обучения с подкреплением — это основа экспериментов. Среди популярных решений — `OpenAI Gym`, `GIMP`, и другие. Мы будем использовать `HighwayFast-v0`, которая моделирует дорожную ситуацию с несколькими полосами и машинами.

Настройки среды включают:

- Количество полос
- Длительность симуляции (например, 40 секунд)
- Начальное расстояние между машинами
- Награды за столкновение (минус 1)
- Диапазон скоростей, при которых агент получает награду (например, 20–30 км/ч)
- Частота симуляции и обновления модели
- Отображение экрана и траектории агента

Все эти параметры можно настроить, чтобы создать нужную среду для обучения.

---

## Выбор алгоритмов и библиотек

Для обучения агента мы будем использовать три алгоритма:

- `DQN` (Deep Q-Network) — алгоритм, основанный на Q-обучении с использованием нейронных сетей.
- `PPO` (Proximal Policy Optimization) — более современный и стабильный алгоритм.
- `A2C` (Advantage Actor-Critic) — метод, сочетающий оценку действий и стратегию.

Все они доступны в библиотеках, таких как `Stable-Baselines3` или `PyTorch`. Не нужно писать алгоритмы с нуля — достаточно выбрать подходящую реализацию и настроить параметры.

---

## Настройка параметров

Каждый алгоритм имеет свои параметры, которые можно настроить для улучшения обучения. Вот основные настройки:

- `Learning starts`: первые 200 шагов агент не обучается, а просто действует
- `Buffer size`: размер буфера памяти, где хранятся предыдущие действия
- `Batch size`: размер батча для обучения
- `Gamma`: коэффициент убывания значимости будущих наград
- `Frequency`: частота обновления модели
- `Epsilon-greedy`: стратегия выбора действий (жадность vs. исследование)

Эти параметры сильно влияют на скорость и качество обучения. Их можно тестировать и подбирать под конкретную задачу.

---

## Обучение и оценка

Обучение происходит в течение 100 000 шагов. Каждые 20 000 шагов сохраняется модель. Мы отслеживаем следующие метрики:

- `Length (средняя длительность поездки)`
- `Reward (средняя награда за эпоху)`
- `AppRewardMin (минимальная награда)`
- `AppEpoch (средняя оценка эпохи)`

Чем выше эти значения, тем лучше агент справляется с задачей.

---

## Сравнение алгоритмов

### DQN

- Обучается медленно, но в итоге обгоняет другие алгоритмы
- Более "рискованный", быстрее едет, но может врезаться
- В конце обучения показывает высокие значения по длительности и награде

### PPO

- Быстрее начинает обучаться
- Стратегия более осторожная, но медленная
- В итоге не сильно отстает от DQN, но не обгоняет его

### A2C

- Плохо справляется с задачей в стандартных настройках
- Стратегия — опускаться в нижний ряд и ехать до столкновения
- Не показывает улучшений даже после 100 000 итераций

---

## Визуализация и TensorBoard

Мы можем визуализировать процесс обучения с помощью `TensorBoard`. Там отображаются графики, показывающие, как меняются метрики с течением времени. Это помогает понять, как агент учится и какие алгоритмы работают лучше.

---

## Заключение

Обучение с подкреплением — это мощный инструмент, но требует правильной настройки среды и алгоритмов. С помощью готовых библиотек и сред, таких как `HighwayFast-v0`, можно быстро запустить эксперименты и протестировать разные подходы.

Главное — не забывать, что выбор алгоритма и настройка параметров — это ключ к успеху. Агент сам принимает решения, основываясь только на наградах и штрафах, что делает обучение с подкреплением по-настоящему интересным и мощным подходом.

---

**P.S.** Если вы хотите углубиться в RL, не забудьте посмотреть, как выглядит обучение в действии. Это помогает лучше понять, как агент принимает решения и как можно улучшить его поведение.