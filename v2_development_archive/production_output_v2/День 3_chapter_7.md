# Градиентный спуск и локальные минимумы: как нейросети учатся на примере кошек и собак

В машинном обучении одна из ключевых задач — минимизация ошибки модели. Но как это происходит на практике? В этом материале мы разберем, как алгоритм **градиентного спуска** помогает нейросетям находить оптимальные параметры, используя простой и понятный пример: классификацию кошек и собак по весу.

---

## Почему один вес не спасает

Представьте, что вы хотите создать модель, которая будет отличать кошек от собак. Вы берете **один признак** — вес. Но тут возникает проблема: мелкие собаки весят почти как кошки. Если модель будет опираться только на вес, то она может разделить их **линией в 5 кг**: все, что тяжелее — собака, легче — кошка. Но это **очень грубая классификация**.

Если модель учится только на одном весе, то её точность будет в районе **70–80%**, и дальше она не улучшится. Это связано с тем, что модель не может учитывать другие важные признаки, например, форму ушей или длину хвоста.

---

## График ошибки и локальные минимумы

Допустим, мы построили график зависимости ошибки от веса. На оси X — значения веса, на Y — ошибка модели. Наша цель — найти **минимальную ошибку**.

![График ошибки от веса](https://via.placeholder.com/600x400?text=График+ошибки+от+веса)

На графике видно, что ошибка минимальна при определённом весе (например, 150), а максимальна при другом (например, 550). Мы инициируем вес в случайной точке, например, в 0. Затем модель начинает **"катиться" вниз**, уменьшая вес или увеличивая его, в зависимости от того, как меняется ошибка.

Этот процесс называется **градиентным спуском**. Он похож на то, как шарик скатывается по холму: он ищет самую низкую точку, но может попасть в **локальный минимум** — ямку, которая не является глобальным минимумом.

---

## Почему важны несколько запусков

Если мы запустим модель из разных начальных точек, то она может попасть в разные локальные минимумы. Например:

- Старт из точки 0 → попадаем в минимум с 42% ошибки.
- Старт из точки 400 → попадаем в тот же минимум.
- Старт из точки 500 → попадаем в худший минимум с 50% ошибки.

Поэтому **лучшая стратегия** — запускать модель несколько раз с разными начальными весами и выбирать тот результат, где ошибка минимальна.

---

## Переходим к нескольким весам

В реальных задачах модели используют **не один, а множество весов**. Например, в нейросети может быть **517 тысяч весов**. Как тогда искать оптимальное решение?

Градиентный спуск помогает **одновременно обновлять все веса**, а не по одному. Это делается с помощью **частных производных**, которые показывают, как изменится ошибка при изменении каждого веса.

В библиотеке `PyTorch` этот процесс реализован автоматически. Вы просто вызываете оптимизатор, например `torch.optim.SGD`, и он сам рассчитывает, как изменить веса, чтобы ошибка уменьшилась.

---

## Как работает градиентный спуск в 2D

Представьте, что у нас есть **два веса**. Тогда график ошибки превращается в **поверхность**. Каждая точка на этой поверхности — это комбинация двух весов и соответствующая ошибка.

Мы как бы **водим точку по этой поверхности**, ищем самую низкую точку. Это похоже на **игру в гольф**: вы выбираете стартовую точку, и мяч катится вниз, пока не попадёт в лунку.

![Поверхность ошибки](https://via.placeholder.com/600x400?text=Поверхность+ошибки)

---

## Шаг обучения и инерция

Один из ключевых параметров градиентного спуска — **шаг обучения**. Он определяет, насколько сильно модель изменит веса на каждом шаге.

- **Слишком большой шаг** → модель может "перепрыгнуть" через минимум и никогда не найдёт оптимальное решение.
- **Слишком маленький шаг** → обучение будет очень медленным.

Существуют и **расширенные алгоритмы**, такие как `Adam`, которые добавляют **инерцию** — модель "помнит" предыдущие шаги и может быстрее сходиться к минимуму.

---

## Зачем запускать модель несколько раз?

Потому что **локальных минимумов бесконечно много**. Если вы запустите модель из разных точек, она может попасть в разные "ямки". Нас интересует **глобальный минимум** — тот, где ошибка минимальна.

Поэтому **лучшая практика** — запускать модель несколько раз с разными начальными весами и выбирать наилучший результат.

---

## Пример с кошками и собаками

Вернёмся к нашему примеру. Представьте, что у нас есть модель, которая классифицирует животных по весу. Веса модели — это коэффициенты, которые определяют, как модель интерпретирует входные данные.

Например, вес может быть `0.175777235`, а другой — `-1.13752`. Эти числа кажутся бессмысленными, но они **важны для модели**, потому что определяют, как она будет принимать решения.

---

## Выводы

- Градиентный спуск — это алгоритм, который помогает модели находить оптимальные веса.
- Он работает, как "катание шарика вниз по холму", ищет минимальную ошибку.
- Модель может попасть в **локальный минимум**, поэтому важно запускать её из разных точек.
- В реальных задачах весов много, и их изменяют **одновременно** с помощью математических методов.
- В `PyTorch` градиентный спуск реализован автоматически, что делает обучение моделей простым и эффективным.

---

## Дополнительно

Если вы хотите поэкспериментировать с градиентным спуском, попробуйте:

- Использовать `PyTorch` и `Pandas` для построения простой модели.
- Визуализировать график ошибки с помощью `matplotlib`.
- Экспериментировать с разными оптимизаторами: `SGD`, `Adam`, `RMSProp`.

Так вы сможете увидеть, как модель "катится" вниз по поверхности ошибки и находит оптимальное решение.