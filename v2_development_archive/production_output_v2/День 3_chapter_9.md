# Прямой проход через слои нейросети: как работает forward-процесс

В этой статье мы разберем, как нейросеть обрабатывает входные данные на этапе **прямого прохода (forward pass)**. Мы рассмотрим, как данные проходят через слои, какие активационные функции используются, и почему архитектура сети — это ключ к её успеху.

## Что происходит на этапе forward-процесса

Представьте, что вы подаете на вход нейросети изображение размером **28x28 пикселей** — это, например, цифра из датасета **MNIST**. Первым делом, изображение преобразуется в вектор, чтобы его можно было подать на вход сети.

### Первый слой: 28x28 → 128 нейронов

- Входной вектор размером 784 (28x28) подается на первый слой, который состоит из **128 нейронов**.
- Каждый нейрон вычисляет линейную комбинацию входных данных и своих весов.
- После этого применяется **ReLU** — активационная функция, которая обнуляет все отрицательные значения, а положительные оставляет без изменений.

> **Пример:** если нейрон выдал значение `-3`, то после ReLU оно станет `0`. Если `5`, то останется `5`.

Результаты этого слоя записываются в переменную `data`, и мы уже не работаем с исходной картинкой — только с активациями нейронов.

### Второй слой: 128 → 64 нейрона

- Далее, те же действия повторяются: умножение на веса, суммирование, применение ReLU.
- Теперь у нас **64 активированных нейрона**.
- Эти значения снова записываются в `data`.

### Третий слой: 64 → 10 нейронов

- В финальном слое у нас **10 нейронов**, соответствующих 10 классам (цифры от 0 до 9).
- Здесь, как правило, **не применяется ReLU**, а вместо этого может использоваться **Softmax** — функция, которая превращает выходы в вероятности (сумма которых равна 1).
- Это позволяет интерпретировать выход как вероятность принадлежности изображения к каждому классу.

> **Важно:** Softmax не является обязательной функцией, но её использование улучшает обучение и интерпретируемость модели.

## Как это выглядит в коде

В PyTorch это можно описать так: