# Предобработка текста и токенизация: от лемматизации к кластеризации

В современных задачах обработки естественного языка (NLP) предобработка текста — один из ключевых этапов, который может сильно повлиять на качество модели. В этом докладе мы разберем, как подготовить текст к анализу, какие шаги используются на практике и как токенизация помогает в построении кластеров тем.

---

## Почему лемматизация важна?

При работе с текстом алгоритмы видят "собака", "собаку", "собаке", "собакой" как разные слова. Это может привести к потере смысла и увеличению размерности данных. **Лемматизация** — это процесс приведения слов к их базовой форме. Например, все формы слова "собака" будут заменены на "собака".

> **Важно:** Лемматизация — не панацея. Иногда она может вредить, особенно если слово в разных контекстах имеет разные значения. Поэтому всегда стоит тестировать и сравнивать результаты.

---

## Токенизация и фильтрация слов

После лемматизации мы переходим к **токенизации** — разбиваем текст на отдельные слова (токены) и присваиваем им индексы. Но не все слова одинаково полезны:

- **Слишком редкие** — появляются один-два раза. Классифицировать по ним невозможно.
- **Слишком частые** — слова вроде "и", "в", "на", которые не несут смысловой нагрузки.

> **Правило:** Удаляем слова, которые встречаются менее 2 раз в документах, и те, которые появляются в более чем 80% документов.

---

## Строим словарь

После фильтрации у нас остается **70 слов**, которые мы пронумеровали по частоте встречаемости. Например:

- 0 — NLP
- 1 — активный
- 2 — анализ
- 3 — текст
- 4 — GPT
- 5 — модель
- и т.д.

Таким образом, мы получаем **векторное представление текста**, где каждое слово — это индекс, а его частота — вес.

---

## Обучаем модель

Мы используем модель, которая ищет **30 тем** в данных. Настройки:

- 20 эпох обучения
- 50 итераций на каждую фразу

> **Минус:** Нужно вручную назвать кластеры. Можно использовать `GPT` для автоматической аннотации.

---

## Как модель определяет темы?

Модель вычисляет **веса слов** в каждом кластере. Сумма весов равна 1, и чем выше вес, тем больше слово характеризует тему.

Пример:

- **Кластер NLP**: NLP (0.154), GPT (0.13), анализ (0.09), обработка (0.08)
- **Кластер временных рядов**: временные ряды (0.144), тренд (0.12), прогнозирование (0.11)
- **Компьютерное зрение**: изображение (0.94), объект (0.85), распознавание (0.78)

> **Замечание:** Анализ может быть общим словом, но в зависимости от контекста оно может относиться к разным темам.

---

## Проверяем точность

Мы протестировали модель на 25 примерах в каждом классе. Средняя точность — **95%**, что впечатляет для такого небольшого датасета.

Примеры:

- "GPT помогает улучшить анализ текста" → NLP
- "Временные ряды используются для предсказания цен" → временные ряды
- "Компьютерное зрение выделяет дорожные знаки" → компьютерное зрение

> **Важно:** Чем больше примеров в классе, тем выше точность. Для 100–200 примеров можно достичь почти 100% точности.

---

## Сравнение с другими методами

Метод главных компонентов (`PCA`) и `k-means` могут давать разные результаты в зависимости от структуры данных. Например, `PCA` лучше справляется с вытянутыми кластерами, в то время как `k-means` ищет круглые.

> **Совет:** Всегда проверяйте, как методы работают на вашем конкретном датасете.

---

## Заключение

Предобработка текста — это не просто механическая процедура. Это искусство, где каждое решение влияет на результат. От лемматизации до токенизации, от фильтрации слов до построения кластеров — все шаги требуют внимания и анализа.

> **Итог:** С помощью правильной предобработки и обучения модели можно достичь высокой точности кластеризации тем, даже при маленьких датасетах.