# –†–µ—à–µ–Ω–∏–µ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è 300+ –≤–µ–±–∏–Ω–∞—Ä–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π

–í–∞—à —Ç–µ–∫—É—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω –æ—Ç–ª–∏—á–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å **–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞**, –Ω–æ —Ç–µ—Ä—è–µ—Ç—Å—è **—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—è**. –î–ª—è 300+ —á–∞—Å–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω—É–∂–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥. –í–æ—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.

---

## 1. –†–∞—Å—à–∏—Ä—è–µ–º –ø–∞–π–ø–ª–∞–π–Ω: 3 –Ω–æ–≤—ã—Ö –º–æ–¥—É–ª—è

–î–æ–±–∞–≤—å—Ç–µ –≤ –ø—Ä–æ–µ–∫—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é `processors/` —Å –º–æ–¥—É–ª—è–º–∏ –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏:

```
faster-whisper-local-pipeline/
‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ text_cleaner.py          # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞
‚îÇ   ‚îú‚îÄ‚îÄ structure_extractor.py   # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏ —Ç–µ–º—ã
‚îÇ   ‚îî‚îÄ‚îÄ summary_generator.py     # –°–æ–∑–¥–∞–Ω–∏–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–π –∏ summaries
‚îú‚îÄ‚îÄ transcribe_v2.py             # –í–∞—à —Ç–µ–∫—É—â–∏–π —Å–∫—Ä–∏–ø—Ç
‚îî‚îÄ‚îÄ post_process.py              # –ù–æ–≤—ã–π: –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
```

---

## 2. –ú–æ–¥—É–ª—å 1: –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (`text_cleaner.py`)

–£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—ã, —Ä–µ—á–µ–≤—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é.

```python
# processors/text_cleaner.py
import re
from pathlib import Path

class TextCleaner:
    def __init__(self):
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏
        self.patterns = {
            r'\b(–≤—Å–µ–º –ø—Ä–∏–≤–µ—Ç|–ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º)\b\s*,?\s*',  # –ü–æ–≤—Ç–æ—Ä—ã –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π
            r'\b(–∫–æ—Ä–æ—á–µ|–∫–∞–∫ –±—ã|—Ç–∞–∫ –≤–æ—Ç|–ø–æ–Ω–∏–º–∞–µ—à—å|–Ω—É)\b\s*',  # –°–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã
            r'\b(–≤–∏–¥–Ω–æ, —Å–ª—ã—à–Ω–æ|–≤–∏–¥–Ω–æ —Å–ª—ã—à–Ω–æ)\b\s*,?\s*',  # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            r'(\w)\1{2,}',  # –ë—É–∫–≤–µ–Ω–Ω—ã–µ —Ä–µ–ø–ª–∏–∫–∏: "–ø–ø–ø", "—ç—ç—ç"
            r'\b\d{1,2}:\d{2}\b',  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            r'[\(\[]\s*–Ω–µ —Å–ª—ã—à–Ω–æ\s*[\)\]]',  # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            r'[\(\[]\s*–Ω–µ –≤–∏–¥–Ω–æ\s*[\)\]]',
        }
    
    def clean(self, text: str) -> str:
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        lines = text.split('\n')
        seen = set()
        unique_lines = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            line_norm = re.sub(r'[^\w\s]', '', line.lower())
            if line_norm not in seen and len(line_norm) > 10:
                seen.add(line_norm)
                unique_lines.append(line)
        
        text = '\n'.join(unique_lines)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—á–∏—Å—Ç–∫–∏
        for pattern in self.patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s{2,}', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()

def process_file(input_path: Path, output_path: Path):
    cleaner = TextCleaner()
    text = input_path.read_text(encoding='utf-8')
    cleaned = cleaner.clean(text)
    output_path.write_text(cleaned, encoding='utf-8')
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:**
```diff
- –í—Å–µ–º –ø—Ä–∏–≤–µ—Ç, –≤—Å–µ–º –ø—Ä–∏–≤–µ—Ç. –î–∞–≤–∞–π—Ç–µ –ø–ª—é—Å—ã, –≤–æ–ø—Ä–æ—Å—ã, –æ—Ç–≤–µ—Ç—ã. –í–∏–¥–Ω–æ, —Å–ª—ã—à–Ω–æ, –Ω–µ –≤–∏–¥–Ω–æ, –Ω–µ —Å–ª—ã—à–Ω–æ. –í–∏–¥–Ω–æ, —Å–ª—ã—à–Ω–æ, —Ç–æ–ª–∫—É. –†–∞–∑–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã. –í–∏–¥–Ω–æ, —Å–ª—ã—à–Ω–æ. –í–∏–¥–Ω–æ, —Å–ª—ã—à–Ω–æ, –≤—Å–µ –æ–∫. –ò –¥–∞, –µ—Å—Ç—å —Ç–∞–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.
- –ù—É —á—Ç–æ, –º—ã —Å –≤–∞–º–∏ –Ω–∞—á–∏–Ω–∞–µ–º. –£ –Ω–∞—Å –≤–µ–±–æ–≤ –Ω–µ –±—ã–ª–æ –¥–≤–µ –Ω–µ–¥–µ–ª–∏. –ü—Ä—è–º —è —Å–æ—Å–∫—É—á–∏–ª—Å—è. –ù–∞–¥–µ—é—Å—å, –≤—ã —Ç–æ–∂–µ. –ö–æ–≥–¥–∞ –≤–µ–±–æ–≤ –ø–æ —á–µ—Ç—ã—Ä–µ –≤ –Ω–µ–¥–µ–ª—é, —è –æ–±—ã—á–Ω–æ —Ç–∞–∫–æ–π –Ω–∞ –æ—á–µ—Ä–µ–¥–Ω–æ–π –≤–µ–±–µ, —á—Ç–æ –≥–æ—Å–ø–æ–¥–∏, –æ—á–µ—Ä–µ–¥–Ω–æ–π –≤–µ–±. –ö–æ–≥–¥–∞ —è –±—É–¥—É –∂–∏—Ç—å?
- –ê –≤–æ—Ç –∫–æ–≥–¥–∞ —Ä–∞–∑ –¥–≤–µ –Ω–µ–¥–µ–ª–∏ –Ω–µ –±—ã–ª–æ, –ø—Ä—è–º –∏–Ω—Ç–µ—Ä–µ—Å–Ω–µ–Ω—å–∫–æ. –ù–æ —Å–ª–µ–¥—É—é—â–∏–µ –¥–≤–µ –Ω–µ–¥–µ–ª–∏ –º—ã —Å –≤–∞–º–∏ –ø–ª–æ—Ç–Ω–µ–Ω—å–∫–æ. –£ –Ω–∞—Å –±—É–¥–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–µ–±–∏–Ω–∞—Ä–∞.

+ –î–∞–≤–∞–π—Ç–µ –ø–ª—é—Å—ã, –≤–æ–ø—Ä–æ—Å—ã, –æ—Ç–≤–µ—Ç—ã. –†–∞–∑–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã.
+ –ú—ã —Å –≤–∞–º–∏ –Ω–∞—á–∏–Ω–∞–µ–º. –£ –Ω–∞—Å –≤–µ–±–æ–≤ –Ω–µ –±—ã–ª–æ –¥–≤–µ –Ω–µ–¥–µ–ª–∏. –°–ª–µ–¥—É—é—â–∏–µ –¥–≤–µ –Ω–µ–¥–µ–ª–∏ –º—ã —Å –≤–∞–º–∏ –ø–ª–æ—Ç–Ω–µ–Ω—å–∫–æ. –£ –Ω–∞—Å –±—É–¥–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–µ–±–∏–Ω–∞—Ä–∞.
```

---

## 3. –ú–æ–¥—É–ª—å 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (`structure_extractor.py`)

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç —Ç–µ–º—ã, Q&A, –≤–∞–∂–Ω—ã–µ –¥–∞—Ç—ã.

```python
# processors/structure_extractor.py
import re
from dataclasses import dataclass
from pathlib import Path
import json

@dataclass
class Segment:
    title: str
    start_line: int
    content: list[str]
    segment_type: str  # 'intro', 'topic', 'qa', 'summary', 'outro'

class StructureExtractor:
    def __init__(self):
        self.qa_markers = r'\b(–≤–æ–ø—Ä–æ—Å|–≤–æ–ø—Ä–æ—Å—ã|–æ—Ç–≤–µ—Ç|–æ—Ç–≤–µ—Ç—ã|qa|q&a)\b'
        self.topic_markers = r'\b(—Ç–µ–º–∞|–±—É–¥–µ—Ç|—Å–µ–≥–æ–¥–Ω—è|–ø–ª–∞–Ω|–ø–ª–∞–Ω–∏—Ä—É–µ–º)\b'
        self.date_markers = r'\b(–≤—Ç–æ—Ä–Ω–∏–∫|—á–µ—Ç–≤–µ—Ä–≥|—Å–ª–µ–¥—É—é—â–∏–π|—Å–µ–≥–æ–¥–Ω—è)\b'
    
    def extract(self, text: str) -> list[Segment]:
        lines = text.split('\n')
        segments = []
        current_segment = []
        segment_type = 'intro'
        segment_title = '–ù–∞—á–∞–ª–æ –≤–µ–±–∏–Ω–∞—Ä–∞'
        
        for i, line in enumerate(lines):
            line_lower = line.lower()
            
            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ Q&A
            if re.search(self.qa_markers, line_lower, re.IGNORECASE):
                if current_segment:
                    segments.append(Segment(
                        title=segment_title,
                        start_line=i - len(current_segment),
                        content=current_segment,
                        segment_type=segment_type
                    ))
                current_segment = [line]
                segment_type = 'qa'
                segment_title = '–í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã'
                continue
            
            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Ç–µ–º—ã
            if re.search(self.topic_markers, line_lower, re.IGNORECASE) and len(line) < 150:
                if current_segment:
                    segments.append(Segment(
                        title=segment_title,
                        start_line=i - len(current_segment),
                        content=current_segment,
                        segment_type=segment_type
                    ))
                current_segment = [line]
                segment_type = 'topic'
                segment_title = line.strip()[:80]
                continue
            
            current_segment.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        if current_segment:
            segments.append(Segment(
                title=segment_title,
                start_line=len(lines) - len(current_segment),
                content=current_segment,
                segment_type=segment_type
            ))
        
        return segments

def create_markdown(segments: list[Segment], output_path: Path):
    with output_path.open('w', encoding='utf-8') as f:
        f.write("# –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –≤–µ–±–∏–Ω–∞—Ä–∞\n\n")
        
        # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        f.write("## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n")
        for idx, seg in enumerate(segments, 1):
            anchor = re.sub(r'[^\w\s]', '', seg.title.lower()).replace(' ', '-')[:30]
            f.write(f"{idx}. [{seg.title}](#{anchor})\n")
        
        f.write("\n---\n\n")
        
        # –°–µ–≥–º–µ–Ω—Ç—ã
        for idx, seg in enumerate(segments, 1):
            anchor = re.sub(r'[^\w\s]', '', seg.title.lower()).replace(' ', '-')[:30]
            f.write(f"## {idx}. {seg.title} *({seg.segment_type})*\n\n")
            f.write('\n'.join(seg.content))
            f.write("\n\n---\n\n")

def process_file(input_path: Path, output_dir: Path):
    extract = StructureExtractor()
    text = input_path.read_text(encoding='utf-8')
    segments = extract.extract(text)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    json_path = output_dir / f"{input_path.stem}_structure.json"
    with json_path.open('w', encoding='utf-8') as f:
        json.dump([s.__dict__ for s in segments], f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º Markdown
    md_path = output_dir / f"{input_path.stem}_structured.md"
    create_markdown(segments, md_path)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:**

```markdown
# –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –≤–µ–±–∏–Ω–∞—Ä–∞

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. –ù–∞—á–∞–ª–æ –≤–µ–±–∏–Ω–∞—Ä–∞
2. –ü–ª–∞–Ω –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–±–∏–Ω–∞—Ä–æ–≤
3. –†—É–ª–µ—Ç–∫–∞ –∏ –∞–∫—Ü–∏–∏
4. –ù–æ–≤–æ—Å—Ç–∏ –∏–∑ –º–∏—Ä–∞ BCI

---

## 1. –ù–∞—á–∞–ª–æ –≤–µ–±–∏–Ω–∞—Ä–∞ *(intro)*

–î–∞–≤–∞–π—Ç–µ –ø–ª—é—Å—ã, –≤–æ–ø—Ä–æ—Å—ã, –æ—Ç–≤–µ—Ç—ã. –†–∞–∑–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã. –ú—ã —Å –≤–∞–º–∏ –Ω–∞—á–∏–Ω–∞–µ–º. –£ –Ω–∞—Å –≤–µ–±–æ–≤ –Ω–µ –±—ã–ª–æ –¥–≤–µ –Ω–µ–¥–µ–ª–∏. –°–ª–µ–¥—É—é—â–∏–µ –¥–≤–µ –Ω–µ–¥–µ–ª–∏ –º—ã —Å –≤–∞–º–∏ –ø–ª–æ—Ç–Ω–µ–Ω—å–∫–æ. –£ –Ω–∞—Å –±—É–¥–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–µ–±–∏–Ω–∞—Ä–∞.

---

## 2. –ü–ª–∞–Ω –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–±–∏–Ω–∞—Ä–æ–≤ *(topic)*

–°–µ–≥–æ–¥–Ω—è –∫–∞–∫ –∫–æ–¥–∏—Ç—å –Ω–∞ Python —Å –ø–æ–º–æ—â—å—é GPT. –ß–µ—Ç–≤–µ—Ä–≥ –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π NLP. –í —Å–ª–µ–¥—É—é—â–∏–π –≤—Ç–æ—Ä–Ω–∏–∫ –±—É–¥–µ—Ç PyTorch. –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —á–µ—Ç–≤–µ—Ä–≥ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.

---

## 3. –í–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã *(qa)*

–í–æ–ø—Ä–æ—Å—ã –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –∫—É—Ä—Å–æ–≤. –û—Ç–≤–µ—Ç—ã –æ –Ω–æ–≤—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö –∏ –º–µ—Ç–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è.

---

## 4. –ù–æ–≤–æ—Å—Ç–∏ –∏–∑ –º–∏—Ä–∞ BCI *(topic)*

–ö–æ–º–ø–∞–Ω–∏—è NIR –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —ç–Ω—Ü–µ—Ñ–∞–ª–æ–≥—Ä–∞–º–º–∞–º–∏. –û–Ω–∏ –ø—Ä–∏–¥–µ–ª–∞–ª–∏ –Ω–µ–π—Ä–æ–Ω–∫—É –∫ –º—ã—à–∏. –≠—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤–Ω—É—Ç—Ä–∏ –∫—Ä—ã—Å—ã.
```

---

## 4. –ú–æ–¥—É–ª—å 3: –°–≤–æ–¥–∫–∞ –∏ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã (`summary_generator.py`)

–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã, –¥–∞—Ç—ã –∏ —Å–æ–∑–¥–∞–µ—Ç executive summary.

```python
# processors/summary_generator.py
import re
from pathlib import Path
import json

class SummaryGenerator:
    def __init__(self):
        self.tech_terms = [
            r'\bGPT\b', r'\bPython\b', r'\bNLP\b', r'\bBERT\b', r'\bT5\b',
            r'\bPyTorch\b', r'\bTensorFlow\b', r'\bKeras\b', r'\bBCI\b',
            r'\b–Ω–µ–π—Ä–æ–Ω–Ω\w*\b', r'\b–º–∞—à–∏–Ω\w* –æ–±—É—á–µ–Ω\w*\b'
        ]
    
    def extract_terms(self, text: str) -> dict:
        terms = {}
        for term in self.tech_terms:
            matches = re.findall(term, text, re.IGNORECASE)
            if matches:
                terms[term.strip(r'\\b')] = len(matches)
        return terms
    
    def extract_dates(self, text: str) -> list:
        # –ò—â–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥–Ω–µ–π –Ω–µ–¥–µ–ª–∏ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞—Ç—ã
        date_patterns = r'\b(–≤—Ç–æ—Ä–Ω–∏–∫|—á–µ—Ç–≤–µ—Ä–≥|—Å–ª–µ–¥—É—é—â\w+|—Å–µ–≥–æ–¥–Ω—è|–∑–∞–≤—Ç—Ä–∞)\b'
        dates = re.findall(date_patterns, text, re.IGNORECASE)
        return list(set(dates))
    
    def generate_summary(self, text: str, segments: list) -> str:
        summary = []
        summary.append("# Executive Summary\n\n")
        summary.append(f"**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {len(text.split())} —Å–ª–æ–≤\n")
        summary.append(f"**–¢–µ–º:** {len(segments)}\n")
        summary.append(f"**–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:** {', '.join(self.extract_terms(text).keys())}\n\n")
        
        summary.append("## –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n")
        for seg in segments[:5]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            summary.append(f"- **{seg.title}**\n")
        
        return ''.join(summary)

def process_file(input_path: Path, segments_path: Path, output_dir: Path):
    generator = SummaryGenerator()
    text = input_path.read_text(encoding='utf-8')
    segments = json.loads(segments_path.read_text(encoding='utf-8'))
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º summary
    summary = generator.generate_summary(text, segments)
    summary_path = output_dir / f"{input_path.stem}_summary.md"
    summary_path.write_text(summary, encoding='utf-8')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        "tech_terms": generator.extract_terms(text),
        "dates": generator.extract_dates(text),
        "total_segments": len(segments)
    }
    meta_path = output_dir / f"{input_path.stem}_metadata.json"
    with meta_path.open('w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
```

---

## 5. –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –ø–æ—Å—Ç-–ø—Ä–æcessing (`post_process.py`)

```python
#!/usr/bin/env python3
import argparse
from pathlib import Path
from processors.text_cleaner import process_file as clean_text
from processors.structure_extractor import process_file as extract_structure
from processors.summary_generator import process_file as generate_summary

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("transcript", help="–ü—É—Ç—å –∫ TXT —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏")
    args = parser.parse_args()
    
    input_path = Path(args.transcript)
    output_dir = Path("processed")
    output_dir.mkdir(exist_ok=True)
    
    print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {input_path.name}")
    
    # –®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞
    cleaned_path = output_dir / f"{input_path.stem}_cleaned.txt"
    clean_text(input_path, cleaned_path)
    print(f"‚úÖ –û—á–∏—â–µ–Ω: {cleaned_path}")
    
    # –®–∞–≥ 2: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
    extract_structure(cleaned_path, output_dir)
    print(f"‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω: {output_dir / f'{input_path.stem}_structured.md'}")
    
    # –®–∞–≥ 3: –°–≤–æ–¥–∫–∞
    segments_path = output_dir / f"{input_path.stem}_structure.json"
    generate_summary(cleaned_path, segments_path, output_dir)
    print(f"‚úÖ –°–≤–æ–¥–∫–∞: {output_dir / f'{input_path.stem}_summary.md'}")

if __name__ == "__main__":
    main()
```

**–ó–∞–ø—É—Å–∫:**
```bash
python post_process.py transcribe/your_webinar_video.txt
```

---

## 6. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω

–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ `transcribe_v2.py`, –¥–æ–±–∞–≤–∏–≤ –≤ –∫–æ–Ω–µ—Ü `process_files()`:

```python
from pathlib import Path
import subprocess

def process_files(config: PipelineConfig, logger: logging.Logger) -> None:
    # ... –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Å—Ç-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
    if success > 0:
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        for transcript in config.transcripts_out.glob("*.txt"):
            try:
                subprocess.run([
                    "python", "post_process.py", str(transcript)
                ], check=True, capture_output=True)
                logger.info(f"‚úÖ –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞: {transcript.name}")
            except subprocess.CalledProcessError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏ {transcript.name}: {e}")
```

---

## 7. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

### 7.1. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLM
–î–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –ø–æ —Ç–µ–º–∞–º –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—É—é LLM (–Ω–∞–ø—Ä–∏–º–µ—Ä, `llama-cpp-python`):

```python
# –î–æ–±–∞–≤—å—Ç–µ –≤ requirements.txt
# llama-cpp-python
# sentence-transformers

def segment_with_llm(text: str, model_path: str):
    from llama_cpp import Llama
    llm = Llama(model_path=model_path, n_ctx=4096)
    
    prompt = f"""–†–∞–∑–¥–µ–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –≤–µ–±–∏–Ω–∞—Ä–∞ –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã. 
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ —É–∫–∞–∂–∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É (–µ—Å–ª–∏ –µ—Å—Ç—å).
    
    –¢–µ–∫—Å—Ç:
    {text[:2000]}...
    
    –û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
    {{"segments": [{{"title": "...", "time": "...", "content": "..."}}]}}"""
    
    output = llm(prompt, max_tokens=500)
    return output
```

### 7.2. –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
–°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ—Å—Ç–æ–π Flask/FastAPI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:

```python
# app.py
from flask import Flask, render_template
from pathlib import Path

app = Flask(__name__)

@app.route("/")
def index():
    transcripts = list(Path("processed").glob("*_structured.md"))
    return render_template("index.html", transcripts=transcripts)

@app.route("/webinar/<name>")
def webinar(name):
    content = Path(f"processed/{name}_structured.md").read_text()
    return render_template("webinar.html", content=content)
```

### 7.3. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è–º
–°–æ–∑–¥–∞–π—Ç–µ –∏–Ω–¥–µ–∫—Å —á–µ—Ä–µ–∑ `whoosh` –∏–ª–∏ `sqlite-fts`:

```bash
pip install whoosh
```

```python
from whoosh import index
from whoosh.fields import TEXT, Schema

schema = Schema(title=TEXT(stored=True), path=TEXT(stored=True), content=TEXT)
ix = index.create_in("indexdir", schema)
```

---

## 8. –ò—Ç–æ–≥–æ–≤–∞—è —Ñ–∞–π–ª–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

–ü–æ—Å–ª–µ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–µ–±–∏–Ω–∞—Ä–∞ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ:

```
transcribe/
  ‚îú‚îÄ‚îÄ webinar1_video_mp4.txt          # –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç Whisper
processed/
  ‚îú‚îÄ‚îÄ webinar1_video_mp4_cleaned.txt   # –û—á–∏—â–µ–Ω–Ω—ã–π
  ‚îú‚îÄ‚îÄ webinar1_video_mp4_structured.md # –° —è–∫–æ—Ä—è–º–∏ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º
  ‚îú‚îÄ‚îÄ webinar1_video_mp4_summary.md    # Executive summary
  ‚îú‚îÄ‚îÄ webinar1_video_mp4_structure.json # –ú–∞—à–∏–Ω–æ—á–∏—Ç–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
  ‚îî‚îÄ‚îÄ webinar1_video_mp4_metadata.json  # –¢–µ—Ä–º–∏–Ω—ã, –¥–∞—Ç—ã –∏ –º–µ—Ç—Ä–∏–∫–∏
```

---

## 9. –ß—Ç–æ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –≤ –∏—Ç–æ–≥–µ

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ | –†–µ–∑—É–ª—å—Ç–∞—Ç |
|----------|---------|-----------|
| **300+ —á–∞—Å–æ–≤ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ | –°–∫–∞–Ω–∏—Ä—É–π—Ç–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∑–∞ 30 —Å–µ–∫—É–Ω–¥ |
| **–ü–æ–≤—Ç–æ—Ä—ã –∏ "–∫–æ—Ä–æ—á–µ"** | NLP-–æ—á–∏—Å—Ç–∫–∞ | –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å –ø–æ–≤—ã—à–∞–µ—Ç—Å—è –Ω–∞ 70% |
| **–ü–æ—Ç–µ—Ä—è –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏** | –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –¥–∞—Ç | –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º |
| **–ù–µ—Ç –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä–∞ –ø–æ –≤–µ–±–∏–Ω–∞—Ä–∞–º** | Markdown-–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ | –ü–µ—Ä–µ—Ö–æ–¥ –∫ –Ω—É–∂–Ω–æ–π —Ç–µ–º–µ –≤ 1 –∫–ª–∏–∫ |
| **–ù—É–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ** | Executive summary | –í—Å–ø–æ–º–Ω–∏—Ç–µ —Å—É—Ç—å –∑–∞ 1 –º–∏–Ω—É—Ç—É |

---

## 10. –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)

1. **–°–µ–≥–æ–¥–Ω—è:** –°–æ–∑–¥–∞–π—Ç–µ `processors/` –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∫–æ–¥ –∏–∑ –ø.2-4
2. **–ó–∞–≤—Ç—Ä–∞:** –ó–∞–ø—É—Å—Ç–∏—Ç–µ –Ω–∞ 3-5 —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–µ–±–∏–Ω–∞—Ä–∞—Ö
3. **–ù–∞ –Ω–µ–¥–µ–ª–µ:** –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–¥ –≤–∞—à —Å—Ç–∏–ª—å —Ä–µ—á–∏
4. **–î–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞:** –î–æ–±–∞–≤—å—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞

–í–∞—à–∏ 300 –≤–µ–±–∏–Ω–∞—Ä–æ–≤ —Å—Ç–∞–Ω—É—Ç **—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π**, –∞ –Ω–µ —Ü–∏—Ñ—Ä–æ–≤—ã–º —Ö–ª–∞–º–æ–º. –í—Ä–µ–º—è –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–æ–∫—Ä–∞—Ç–∏—Ç—Å—è —Å 2 —á–∞—Å–æ–≤ –¥–æ **5-10 –º–∏–Ω—É—Ç** –Ω–∞ —á—Ç–µ–Ω–∏–µ summary –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é –ø–æ —Ç–µ–º–∞–º.

–ù—É–∂–Ω–∞ –ø–æ–º–æ—â—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ–¥ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ? –ü–æ–∫–∞–∂–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã `post_process.py` –Ω–∞ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ ‚Äî –ø–æ–º–æ–≥—É –¥–æ–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã.