# –†–µ—à–µ–Ω–∏–µ: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤

–í–∞—à–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Ç–∏–ø–∏—á–Ω–∞ –¥–ª—è raw —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤! –í–æ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏—è —Ö–∞–æ—Ç–∏—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ–ª–µ–∑–Ω—É—é knowledge base.

## üéØ –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
Raw Transcript ‚Üí AI Processing ‚Üí Structured Output ‚Üí Vector DB ‚Üí Search/RAG
```

---

## üì¶ –†–µ—à–µ–Ω–∏–µ 1: Post-Processing Pipeline (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞—á–∞—Ç—å —Å —ç—Ç–æ–≥–æ)

–î–æ–±–∞–≤—å—Ç–µ –≤ –ø—Ä–æ–µ–∫—Ç –º–æ–¥—É–ª—å –¥–ª—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM.

### –ù–æ–≤—ã–π —Ñ–∞–π–ª: `postprocess.py`

```python
"""
–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤: –æ—á–∏—Å—Ç–∫–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
"""

from __future__ import annotations

import json
import logging
import re
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Optional

try:
    import anthropic
except ImportError:
    anthropic = None

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None


@dataclass
class ProcessedTranscript:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞."""
    
    title: str
    summary: str
    key_topics: list[str]
    sections: list[dict[str, str]]  # [{"title": "...", "content": "...", "timestamp": "..."}]
    key_quotes: list[str]
    action_items: list[str]
    metadata: dict[str, str | list[str]]


class TranscriptProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM."""
    
    def __init__(
        self,
        provider: str = "anthropic",  # "anthropic", "openai", –∏–ª–∏ "local"
        model: str = "claude-3-5-sonnet-20241022",
        api_key: Optional[str] = None,
        logger: Optional[logging.Logger] = None,
    ):
        self.provider = provider
        self.model = model
        self.logger = logger or logging.getLogger(__name__)
        
        if provider == "anthropic":
            if anthropic is None:
                raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install anthropic")
            self.client = anthropic.Anthropic(api_key=api_key)
        elif provider == "openai":
            if OpenAI is None:
                raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            self.client = OpenAI(api_key=api_key)
        else:
            self.client = None  # –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    def _create_processing_prompt(self, raw_text: str) -> str:
        """–°–æ–∑–¥–∞—ë—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞."""
        return f"""–¢—ã –ø–æ–ª—É—á–∏–ª RAW —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —É—á–µ–±–Ω–æ–≥–æ –≤–µ–±–∏–Ω–∞—Ä–∞/–ª–µ–∫—Ü–∏–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –µ–≥–æ –≤ —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª.

–ò–°–•–û–î–ù–´–ô –¢–†–ê–ù–°–ö–†–ò–ü–¢:
{raw_text[:30000]}  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–ó–ê–î–ê–ß–ò:
1. –£–¥–∞–ª–∏ verbal fillers (—ç-—ç, –º-–º, –Ω—É, —Ç–∏–ø–∞, –∫–æ—Ä–æ—á–µ) –∏ –ø–æ–≤—Ç–æ—Ä—ã
2. –†–∞–∑–±–µ–π –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
3. –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã (keywords)
4. –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
5. –ò–∑–≤–ª–µ–∫–∏ –≤–∞–∂–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏ —Ñ–∞–∫—Ç—ã
6. –ù–∞–π–¥–∏ action items / —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
{{
  "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–µ–±–∏–Ω–∞—Ä–∞",
  "summary": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è",
  "key_topics": ["—Ç–µ–º–∞1", "—Ç–µ–º–∞2", ...],
  "sections": [
    {{"title": "–†–∞–∑–¥–µ–ª 1", "content": "–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–∞–∑–¥–µ–ª–∞", "timestamp": "–ø—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è"}},
    ...
  ],
  "key_quotes": ["—Ü–∏—Ç–∞—Ç–∞1", "—Ü–∏—Ç–∞—Ç–∞2", ...],
  "action_items": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è1", ...],
  "metadata": {{
    "difficulty": "beginner|intermediate|advanced",
    "duration": "–ø—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
    "technologies": ["Python", "PyTorch", ...]
  }}
}}

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–æ –∏ –ø–æ—Å–ª–µ."""

    def _call_llm(self, prompt: str) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç LLM API."""
        if self.provider == "anthropic":
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4096,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        
        elif self.provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4096,
            )
            return response.choices[0].message.content
        
        else:
            raise NotImplementedError(f"Provider '{self.provider}' not implemented")
    
    def process_transcript(self, raw_text: str) -> ProcessedTranscript:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç."""
        self.logger.info("–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ (–¥–ª–∏–Ω–∞: %d —Å–∏–º–≤–æ–ª–æ–≤)", len(raw_text))
        
        prompt = self._create_processing_prompt(raw_text)
        
        try:
            response = self._call_llm(prompt)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–∏–ª–∞ —Ç–µ–∫—Å—Ç)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                response = json_match.group(0)
            
            data = json.loads(response)
            
            return ProcessedTranscript(
                title=data.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                summary=data.get("summary", ""),
                key_topics=data.get("key_topics", []),
                sections=data.get("sections", []),
                key_quotes=data.get("key_quotes", []),
                action_items=data.get("action_items", []),
                metadata=data.get("metadata", {}),
            )
        
        except json.JSONDecodeError as exc:
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON –æ—Ç LLM: %s", exc)
            self.logger.debug("–û—Ç–≤–µ—Ç LLM: %s", response[:500])
            raise
        except Exception as exc:
            self.logger.exception("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: %s", exc)
            raise
    
    def save_processed(
        self,
        processed: ProcessedTranscript,
        output_dir: Path,
        filename_stem: str,
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö."""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # JSON (–º–∞—à–∏–Ω–æ—á–∏—Ç–∞–µ–º—ã–π)
        json_path = output_dir / f"{filename_stem}_processed.json"
        with json_path.open("w", encoding="utf-8") as f:
            json.dump(asdict(processed), f, ensure_ascii=False, indent=2)
        self.logger.info("–°–æ—Ö—Ä–∞–Ω—ë–Ω JSON: %s", json_path)
        
        # Markdown (—á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π)
        md_path = output_dir / f"{filename_stem}_structured.md"
        with md_path.open("w", encoding="utf-8") as f:
            f.write(f"# {processed.title}\n\n")
            f.write(f"## üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n{processed.summary}\n\n")
            
            if processed.key_topics:
                f.write(f"## üîë –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã\n\n")
                for topic in processed.key_topics:
                    f.write(f"- {topic}\n")
                f.write("\n")
            
            f.write(f"## üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n")
            for i, section in enumerate(processed.sections, 1):
                title = section.get("title", f"–†–∞–∑–¥–µ–ª {i}")
                content = section.get("content", "")
                timestamp = section.get("timestamp", "")
                
                f.write(f"### {title}")
                if timestamp:
                    f.write(f" `[{timestamp}]`")
                f.write("\n\n")
                f.write(f"{content}\n\n")
            
            if processed.key_quotes:
                f.write(f"## üí° –í–∞–∂–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã\n\n")
                for quote in processed.key_quotes:
                    f.write(f"> {quote}\n\n")
            
            if processed.action_items:
                f.write(f"## ‚úÖ Action Items\n\n")
                for item in processed.action_items:
                    f.write(f"- [ ] {item}\n")
        
        self.logger.info("–°–æ—Ö—Ä–∞–Ω—ë–Ω Markdown: %s", md_path)


def process_all_transcripts(
    input_dir: Path,
    output_dir: Path,
    processor: TranscriptProcessor,
    logger: logging.Logger,
) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
    txt_files = list(input_dir.glob("*.txt"))
    
    if not txt_files:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ TXT —Ñ–∞–π–ª–æ–≤ –≤ %s", input_dir)
        return
    
    logger.info("–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: %d", len(txt_files))
    
    for i, txt_file in enumerate(txt_files, 1):
        logger.info("[%d/%d] –û–±—Ä–∞–±–æ—Ç–∫–∞: %s", i, len(txt_files), txt_file.name)
        
        try:
            raw_text = txt_file.read_text(encoding="utf-8")
            processed = processor.process_transcript(raw_text)
            processor.save_processed(processed, output_dir, txt_file.stem)
            logger.info("‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: %s", txt_file.name)
        
        except Exception as exc:
            logger.error("‚úó –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ %s: %s", txt_file.name, exc)
            continue


def main() -> None:
    """CLI –¥–ª—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    import os
    from transcribe_v2 import configure_logging
    
    logger = configure_logging()
    
    api_key = os.getenv("ANTHROPIC_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ANTHROPIC_API_KEY –∏–ª–∏ OPENAI_API_KEY")
        return
    
    provider = "anthropic" if os.getenv("ANTHROPIC_API_KEY") else "openai"
    model = os.getenv("LLM_MODEL", "claude-3-5-sonnet-20241022" if provider == "anthropic" else "gpt-4o")
    
    processor = TranscriptProcessor(
        provider=provider,
        model=model,
        api_key=api_key,
        logger=logger,
    )
    
    input_dir = Path(os.getenv("TRANSCRIBE_OUT", "transcribe"))
    output_dir = Path(os.getenv("PROCESSED_OUT", "processed"))
    
    process_all_transcripts(input_dir, output_dir, processor, logger)
    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ %s", output_dir)


if __name__ == "__main__":
    main()
```

### –û–±–Ω–æ–≤–∏—Ç–µ `.env`:

```bash
# –î–æ–±–∞–≤—å—Ç–µ –≤ .env
ANTHROPIC_API_KEY=your_key_here
# –∏–ª–∏
OPENAI_API_KEY=your_key_here

LLM_MODEL=claude-3-5-sonnet-20241022
PROCESSED_OUT=processed
```

### –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:

```bash
pip install anthropic  # –∏–ª–∏ openai
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:

```bash
# –°–Ω–∞—á–∞–ª–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
python transcribe_v2.py

# –ó–∞—Ç–µ–º –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
python postprocess.py
```

---

## üì¶ –†–µ—à–µ–Ω–∏–µ 2: RAG Knowledge Base (–¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ 300 –≤–µ–±–∏–Ω–∞—Ä–∞–º)

### –ù–æ–≤—ã–π —Ñ–∞–π–ª: `knowledge_base.py`

```python
"""
–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞–º.
"""

from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Optional

try:
    import chromadb
    from chromadb.utils import embedding_functions
except ImportError:
    chromadb = None


class TranscriptKnowledgeBase:
    """–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞–º."""
    
    def __init__(
        self,
        db_path: Path = Path("chroma_db"),
        collection_name: str = "webinars",
        logger: Optional[logging.Logger] = None,
    ):
        if chromadb is None:
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install chromadb")
        
        self.logger = logger or logging.getLogger(__name__)
        self.client = chromadb.PersistentClient(path=str(db_path))
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º sentence-transformers –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="intfloat/multilingual-e5-large"  # —Ö–æ—Ä–æ—à –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
        )
        
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            embedding_function=embedding_fn,
            metadata={"description": "Webinar transcripts knowledge base"}
        )
        
        self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –±–∞–∑–∞: %s (–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: %d)", 
                        collection_name, self.collection.count())
    
    def index_processed_transcript(self, json_path: Path) -> None:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –∏–∑ JSON."""
        data = json.loads(json_path.read_text(encoding="utf-8"))
        
        doc_id = json_path.stem
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ä–∞–∑–¥–µ–ª—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        for i, section in enumerate(data.get("sections", [])):
            section_id = f"{doc_id}_section_{i}"
            
            metadata = {
                "source": json_path.name,
                "title": data.get("title", ""),
                "section_title": section.get("title", ""),
                "timestamp": section.get("timestamp", ""),
                "topics": ",".join(data.get("key_topics", [])),
            }
            
            content = f"{section.get('title', '')}\n\n{section.get('content', '')}"
            
            self.collection.upsert(
                ids=[section_id],
                documents=[content],
                metadatas=[metadata],
            )
        
        self.logger.info("–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω: %s (%d —Ä–∞–∑–¥–µ–ª–æ–≤)", 
                        data.get("title"), len(data.get("sections", [])))
    
    def index_directory(self, processed_dir: Path) -> None:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ JSON —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        json_files = list(processed_dir.glob("*_processed.json"))
        
        self.logger.info("–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: %d", len(json_files))
        
        for json_file in json_files:
            try:
                self.index_processed_transcript(json_file)
            except Exception as exc:
                self.logger.error("–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ %s: %s", json_file.name, exc)
    
    def search(
        self,
        query: str,
        n_results: int = 5,
        filter_topics: Optional[list[str]] = None,
    ) -> list[dict]:
        """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã."""
        where = None
        if filter_topics:
            # –ü—Ä–æ—Å—Ç–æ–π —Ñ–∏–ª—å—Ç—Ä (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
            where = {"topics": {"$contains": filter_topics[0]}}
        
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results,
            where=where,
        )
        
        formatted = []
        for i in range(len(results["ids"][0])):
            formatted.append({
                "content": results["documents"][0][i],
                "metadata": results["metadatas"][0][i],
                "distance": results["distances"][0][i],
            })
        
        return formatted


def main() -> None:
    """CLI –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞."""
    import os
    from transcribe_v2 import configure_logging
    
    logger = configure_logging()
    
    kb = TranscriptKnowledgeBase(logger=logger)
    
    processed_dir = Path(os.getenv("PROCESSED_OUT", "processed"))
    
    if processed_dir.exists():
        kb.index_directory(processed_dir)
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
    print("\nüîç –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π (Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞)\n")
    
    while True:
        try:
            query = input("–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            if not query:
                continue
            
            results = kb.search(query, n_results=3)
            
            print(f"\nüìö –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}\n")
            
            for i, result in enumerate(results, 1):
                print(f"[{i}] {result['metadata'].get('title', 'N/A')}")
                print(f"    –†–∞–∑–¥–µ–ª: {result['metadata'].get('section_title', 'N/A')}")
                print(f"    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {1 - result['distance']:.2%}")
                print(f"\n{result['content'][:300]}...\n")
                print("-" * 80)
        
        except KeyboardInterrupt:
            print("\n\n–î–æ –≤—Å—Ç—Ä–µ—á–∏!")
            break


if __name__ == "__main__":
    main()
```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞:

```bash
pip install chromadb sentence-transformers
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:

```bash
# –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤
python knowledge_base.py

# –ó–∞—Ç–µ–º –º–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ
```

---

## üöÄ –ü–æ–ª–Ω—ã–π Pipeline

### –û–±–Ω–æ–≤–∏—Ç–µ `requirements.txt`:

```
faster-whisper>=1.0.0
python-dotenv
pytest
anthropic  # –∏–ª–∏ openai
chromadb
sentence-transformers
```

### –°–æ–∑–¥–∞–π—Ç–µ `pipeline.py` (all-in-one):

```python
"""
–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è ‚Üí —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Üí –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è.
"""

import os
from pathlib import Path
from transcribe_v2 import PipelineConfig, process_files, configure_logging
from postprocess import TranscriptProcessor, process_all_transcripts
from knowledge_base import TranscriptKnowledgeBase


def run_full_pipeline() -> None:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤–µ—Å—å pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    logger = configure_logging()
    
    logger.info("=" * 60)
    logger.info("–≠–¢–ê–ü 1: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –≤–∏–¥–µ–æ/–∞—É–¥–∏–æ")
    logger.info("=" * 60)
    
    config = PipelineConfig.from_env()
    process_files(config, logger)
    
    logger.info("\n" + "=" * 60)
    logger.info("–≠–¢–ê–ü 2: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤")
    logger.info("=" * 60)
    
    api_key = os.getenv("ANTHROPIC_API_KEY") or os.getenv("OPENAI_API_KEY")
    if api_key:
        provider = "anthropic" if os.getenv("ANTHROPIC_API_KEY") else "openai"
        model = os.getenv("LLM_MODEL", "claude-3-5-sonnet-20241022")
        
        processor = TranscriptProcessor(
            provider=provider,
            model=model,
            api_key=api_key,
            logger=logger,
        )
        
        input_dir = Path(os.getenv("TRANSCRIBE_OUT", "transcribe"))
        output_dir = Path(os.getenv("PROCESSED_OUT", "processed"))
        
        process_all_transcripts(input_dir, output_dir, processor, logger)
    else:
        logger.warning("–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–µ—Ç API –∫–ª—é—á–∞). –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ANTHROPIC_API_KEY –∏–ª–∏ OPENAI_API_KEY")
    
    logger.info("\n" + "=" * 60)
    logger.info("–≠–¢–ê–ü 3: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î")
    logger.info("=" * 60)
    
    try:
        kb = TranscriptKnowledgeBase(logger=logger)
        processed_dir = Path(os.getenv("PROCESSED_OUT", "processed"))
        
        if processed_dir.exists():
            kb.index_directory(processed_dir)
            logger.info("‚úì –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ knowledge_base.py –¥–ª—è –ø–æ–∏—Å–∫–∞")
        else:
            logger.warning("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è %s –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", processed_dir)
    
    except ImportError:
        logger.warning("–ü—Ä–æ–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (chromadb –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)")
    
    logger.info("\n" + "=" * 60)
    logger.info("‚úÖ PIPELINE –ó–ê–í–ï–†–®–Å–ù")
    logger.info("=" * 60)


if __name__ == "__main__":
    run_full_pipeline()
```

### –ó–∞–ø—É—Å–∫ –≤—Å–µ–≥–æ pipeline:

```bash
python pipeline.py
```

---

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç

–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∞—à–µ–≥–æ –æ–±—Ä–∞–∑—Ü–∞ –ø–æ–ª—É—á–∏—Ç—Å—è:

### `meeting_video_mp4_structured.md`:

```markdown
# –í–µ–±–∏–Ω–∞—Ä: –ö–∞–∫ –∫–æ–¥–∏—Ç—å –Ω–∞ Python —Å –ø–æ–º–æ—â—å—é GPT

## üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

–í–µ–±–∏–Ω–∞—Ä –ø–æ—Å–≤—è—â—ë–Ω –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é GPT –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Python. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞, –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã NLP, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –æ–±–ª–∞—Å—Ç–∏ –Ω–µ–π—Ä–æ–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤.

## üîë –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã

- GPT –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
- –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π NLP (BERT, T5)
- PyTorch
- –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
- –ù–µ–π—Ä–æ–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã

## üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

### –í–≤–µ–¥–µ–Ω–∏–µ –∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ `[00:00]`

–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –∞–Ω–æ–Ω—Å –ø—Ä–æ–≥—Ä–∞–º–º—ã –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –¥–≤–µ –Ω–µ–¥–µ–ª–∏. –ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è —á–µ—Ç—ã—Ä–µ –≤–µ–±–∏–Ω–∞—Ä–∞ –ø–æ —Ç–µ–º–∞–º: –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å GPT, –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π NLP, PyTorch –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.

### –ü—Ä–æ–º–æ-–∞–∫—Ü–∏—è –∏ –Ω–æ–≤–æ—Å—Ç–∏ `[05:30]`

–ê–Ω–æ–Ω—Å –Ω–æ–≤–æ–π —Ä—É–ª–µ—Ç–∫–∏ –≤ Telegram-–±–æ—Ç–µ –¥–ª—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∫—É—Ä—Å–∞...

### –ù–æ–≤–æ—Å—Ç—å: –Ω–µ–π—Ä–æ–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã `[10:15]`

–†–∞—Å—Å–∫–∞–∑ –æ –∫–æ–º–ø–∞–Ω–∏–∏ NIR, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —ç–Ω—Ü–µ—Ñ–∞–ª–æ–≥—Ä–∞–º–º–∞–º–∏ –∏ –Ω–µ–π—Ä–æ–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –û–ø–∏—Å–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –º—ã—à—å—é, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—É—á–∏–ª–∞—Å—å –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ –ø—Ä—è–º—É—é —Å—Ç–∏–º—É–ª—è—Ü–∏—é –º–æ–∑–≥–∞. LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã...

## üí° –í–∞–∂–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã

> "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º ‚Äî —ç—Ç–æ –∫–æ–≥–¥–∞ –Ω–µ–π—Ä–æ–Ω–∫–∞ –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ä–µ–¥–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"

> "–î–æ—Ñ–∞–º–∏–Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ ‚Äî —ç—Ç–æ –º–æ—â–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏"

## ‚úÖ Action Items

- [ ] –ò–∑—É—á–∏—Ç—å PyTorch –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
- [ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö NLP –º–æ–¥–µ–ª–µ–π (BERT, T5) –¥–ª—è —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á
```

---

## üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏

**Claude 3.5 Sonnet:**
- –í—Ö–æ–¥: $3 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- –í—ã—Ö–æ–¥: $15 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- –û–¥–∏–Ω –≤–µ–±–∏–Ω–∞—Ä (~30K —Ç–æ–∫–µ–Ω–æ–≤): ~$0.10-0.20

**GPT-4o:**
- –í—Ö–æ–¥: $2.50 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤  
- –í—ã—Ö–æ–¥: $10 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- –û–¥–∏–Ω –≤–µ–±–∏–Ω–∞—Ä: ~$0.08-0.15

**–î–ª—è 300 –≤–µ–±–∏–Ω–∞—Ä–æ–≤: $24-60** (–µ–¥–∏–Ω–æ—Ä–∞–∑–æ–≤–æ)

---

## üéÅ –ë–æ–Ω—É—Å: –õ–æ–∫–∞–ª—å–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **Ollama** —Å `llama3.2` –∏–ª–∏ `mistral`:

```python
def _call_local_llm(self, prompt: str) -> str:
    """–í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Ollama."""
    import requests
    
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "llama3.2",
            "prompt": prompt,
            "stream": False,
        }
    )
    return response.json()["response"]
```

---

## üìà –ß—Ç–æ –≤ –∏—Ç–æ–≥–µ?

1. ‚úÖ **–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã** –≤ Markdown
2. ‚úÖ **–ú–∞—à–∏–Ω–æ—á–∏—Ç–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ** –≤ JSON
3. ‚úÖ **–ü–æ–∏—Å–∫ –ø–æ 300 –≤–µ–±–∏–Ω–∞—Ä–∞–º** –∑–∞ —Å–µ–∫—É–Ω–¥—ã
4. ‚úÖ **–≠–∫—Å–ø–æ—Ä—Ç –≤ Notion/Obsidian** (Markdown)
5. ‚úÖ **RAG-—á–∞—Ç–±–æ—Ç** –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –∫—É—Ä—Å—É

–ó–∞–ø—É—Å–∫–∞–π—Ç–µ `python pipeline.py` –∏ –ø–æ–ª—É—á–∞–π—Ç–µ –ø–æ–ª–µ–∑–Ω—É—é knowledge base –∏–∑ –≤—Å–µ—Ö –≤–µ–±–∏–Ω–∞—Ä–æ–≤! üöÄ