–û—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–µ–∫—Ç! –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–ª–æ—Ö–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –æ—á–µ–Ω—å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∞. –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:

## üéØ –ü—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∏ —Ä–µ—à–µ–Ω–∏—è

### 1. **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≥–æ–≤–æ—Ä—è—â–∏–º**

```python
# –î–æ–±–∞–≤—å—Ç–µ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
@dataclass(slots=True)
class EnhancedPipelineConfig(PipelineConfig):
    segment_duration: int = 30  # —Å–µ–∫—É–Ω–¥—ã
    speaker_diarization: bool = False
    min_silence_duration: int = 1000  # –º—Å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≥–æ–≤–æ—Ä—è—â–∏—Ö
    output_format: str = "structured"  # structured, srt, txt
```

### 2. **–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞**

–°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥—É–ª—å `text_postprocessor.py`:

```python
import re
from typing import List, Dict
import spacy
from collections import defaultdict

class TextPostProcessor:
    def __init__(self):
        try:
            self.nlp = spacy.load("ru_core_news_sm")
        except OSError:
            self.nlp = None
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑
        text = re.sub(r'(\b\w+\b)(?:\s+\1)+', r'\1', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –º–µ–∂–¥–æ–º–µ—Ç–∏–π –∏ —Å–ª–æ–≤-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤
        filler_words = {'–Ω—É', '–∫–æ—Ä–æ—á–µ', '—Ç–∏–ø–∞', '–≤–æ—Ç', '—ç—Ç–æ', '–∫–∞–∫ –±—ã'}
        words = text.split()
        words = [w for w in words if w.lower() not in filler_words]
        
        return ' '.join(words)
    
    def split_into_paragraphs(self, text: str, max_length: int = 500) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –∞–±–∑–∞—Ü—ã –ø–æ —Å–º—ã—Å–ª—É"""
        if not self.nlp:
            # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–æ—á–∫–∞–º
            sentences = re.split(r'[.!?]+', text)
            paragraphs = []
            current_para = []
            current_len = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                if current_len + len(sentence) > max_length and current_para:
                    paragraphs.append(' '.join(current_para))
                    current_para = []
                    current_len = 0
                
                current_para.append(sentence)
                current_len += len(sentence)
            
            if current_para:
                paragraphs.append(' '.join(current_para))
            
            return paragraphs
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ spacy –¥–ª—è –±–æ–ª–µ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        doc = self.nlp(text)
        paragraphs = []
        current_para = []
        current_len = 0
        
        for sent in doc.sents:
            sent_text = sent.text.strip()
            if not sent_text:
                continue
                
            if current_len + len(sent_text) > max_length and current_para:
                paragraphs.append(' '.join(current_para))
                current_para = []
                current_len = 0
            
            current_para.append(sent_text)
            current_len += len(sent_text)
        
        if current_para:
            paragraphs.append(' '.join(current_para))
        
        return paragraphs
    
    def extract_key_phrases(self, text: str, top_n: int = 10) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑"""
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', text.lower())
        word_freq = defaultdict(int)
        
        for word in words:
            if len(word) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                word_freq[word] += 1
        
        return sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]
```

### 3. **–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏**

–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏:

```python
def enhanced_transcribe_audio(
    job: TranscriptionJob,
    output_path: Path,
    model: WhisperModel,
    config: EnhancedPipelineConfig,
    logger: logging.Logger,
) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    logger.info("–ù–∞—á–∞–ª–æ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: %s", job.audio_path.name)
    start_time = time.perf_counter()

    try:
        segments, info = model.transcribe(
            str(job.audio_path),
            beam_size=config.beam_size,
            vad_filter=config.vad_filter,
            vad_parameters={"min_silence_duration_ms": config.vad_min_silence_ms},
        )

        post_processor = TextPostProcessor()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        structured_segments = []
        full_text = ""
        
        for segment in segments:
            text = segment.text.strip()
            if text:
                structured_segments.append({
                    'start': segment.start,
                    'end': segment.end,
                    'text': text,
                    'cleaned_text': post_processor.clean_text(text)
                })
                full_text += text + " "

        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
        paragraphs = post_processor.split_into_paragraphs(full_text)
        key_phrases = post_processor.extract_key_phrases(full_text)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        output_data = {
            'metadata': {
                'source_file': job.source_path.name,
                'audio_file': job.audio_path.name,
                'language': info.language,
                'language_probability': info.language_probability,
                'duration': info.duration,
                'processing_time': time.perf_counter() - start_time
            },
            'key_phrases': [phrase for phrase, count in key_phrases],
            'structured_content': {
                'paragraphs': paragraphs,
                'segments': structured_segments
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        if config.output_format == "json":
            with output_path.with_suffix('.json').open('w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        elif config.output_format == "structured":
            # Markdown-—Ñ–æ—Ä–º–∞—Ç —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
            with output_path.with_suffix('.md').open('w', encoding='utf-8') as f:
                f.write(f"# –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è: {job.source_path.name}\n\n")
                f.write(f"**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {info.duration:.2f} —Å–µ–∫\n")
                f.write(f"**–Ø–∑—ã–∫:** {info.language} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {info.language_probability:.2f})\n")
                f.write(f"**–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã:** {', '.join(output_data['key_phrases'][:5])}\n\n")
                
                f.write("## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n")
                for i, para in enumerate(paragraphs, 1):
                    f.write(f"{i}. {para[:100]}...\n")
                
                f.write("\n## –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç\n\n")
                for i, para in enumerate(paragraphs, 1):
                    f.write(f"### –ê–±–∑–∞—Ü {i}\n\n{para}\n\n")
        
        elif config.output_format == "srt":
            # –°—É–±—Ç–∏—Ç—Ä—ã
            with output_path.with_suffix('.srt').open('w', encoding='utf-8') as f:
                for i, segment in enumerate(structured_segments, 1):
                    start_time = format_timestamp(segment['start'])
                    end_time = format_timestamp(segment['end'])
                    f.write(f"{i}\n{start_time} --> {end_time}\n{segment['text']}\n\n")
        
        else:  # txt –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            with output_path.open('w', encoding='utf-8') as f:
                f.write(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è: {job.source_path.name}\n")
                f.write("=" * 50 + "\n\n")
                for para in paragraphs:
                    f.write(para + "\n\n")

        elapsed = time.perf_counter() - start_time
        logger.info(
            "–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: %s (%.2f —Å–µ–∫)",
            job.audio_path.name,
            elapsed,
        )
        return True
        
    except Exception as exc:
        logger.exception("–û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ %s", job.audio_path.name)
        return False

def format_timestamp(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è SRT"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace('.', ',')
```

### 4. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö**

–î–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞:

```python
# requirements_additional.txt
# chromadb>=0.4.0
# sentence-transformers>=2.2.0

class KnowledgeBase:
    def __init__(self, persist_directory: str = "./knowledge_base"):
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(exist_ok=True)
        
    def store_transcription(self, transcription_data: dict, source_info: dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"""
        try:
            import chromadb
            from sentence_transformers import SentenceTransformer
            
            client = chromadb.PersistentClient(path=str(self.persist_directory))
            collection = client.get_or_create_collection("webinar_transcriptions")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            
            embeddings = model.encode(transcription_data['full_text'])
            
            collection.add(
                embeddings=[embeddings.tolist()],
                documents=[transcription_data['full_text']],
                metadatas=[{
                    'source_file': source_info['source_file'],
                    'duration': source_info['duration'],
                    'language': source_info['language'],
                    'timestamp': time.time(),
                    'key_phrases': transcription_data['key_phrases']
                }],
                ids=[source_info['source_file']]
            )
            
        except ImportError:
            print("–î–ª—è —Ä–∞–±–æ—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ chromadb –∏ sentence-transformers")
```

### 5. **–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç**

```python
def process_enhanced_files(config: EnhancedPipelineConfig, logger: logging.Logger) -> None:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    logger.info("–ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ Faster-Whisper Local")
    ensure_directories(config)
    
    if media_requires_ffmpeg(config):
        try:
            ensure_ffmpeg_available(logger)
        except RuntimeError as exc:
            logger.error("–ù—É–∂–µ–Ω FFmpeg –¥–ª—è –≤–∏–¥–µ–æ/—Å–∂–∞—Ç—ã—Ö –∞—É–¥–∏–æ: %s", exc)
            return

    jobs = prepare_jobs(config, logger)
    if not jobs:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    logger.info("–ù–∞–π–¥–µ–Ω–æ –∑–∞–¥–∞—á –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: %d", len(jobs))

    try:
        model = load_model(config, logger)
    except Exception as exc:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Whisper: %s", exc)
        return

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    knowledge_base = KnowledgeBase()
    
    success = 0
    for index, job in enumerate(jobs, start=1):
        logger.info("[%d/%d] –û–±—Ä–∞–±–æ—Ç–∫–∞ %s", index, len(jobs), job.audio_path.name)
        
        result_path = config.transcripts_out / f"{job.audio_path.stem}.{config.output_format}"
        if result_path.exists() and result_path.stat().st_mtime >= job.audio_path.stat().st_mtime:
            logger.info("–ü—Ä–æ–ø—É—Å–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (–∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç): %s", result_path.name)
            success += 1
            continue
            
        if enhanced_transcribe_audio(job, result_path, model, config, logger):
            success += 1
            logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: %s", result_path.name)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if config.enable_knowledge_base:
                try:
                    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
                    with result_path.with_suffix('.json').open('r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    knowledge_base.store_transcription(
                        data['structured_content'],
                        data['metadata']
                    )
                    logger.info("–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
                except Exception as e:
                    logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: %s", e)
        else:
            logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ %s", job.audio_path.name)

    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: %d/%d —Ñ–∞–π–ª–æ–≤ —É—Å–ø–µ—à–Ω–æ", success, len(jobs))
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —É–ª—É—á—à–µ–Ω–∏–π

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**
```bash
pip install spacy sentence-transformers chromadb
python -m spacy download ru_core_news_sm
```

2. **–û–±–Ω–æ–≤–∏—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ `.env`:**
```env
# –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
OUTPUT_FORMAT=structured  # structured, srt, json, txt
ENABLE_KNOWLEDGE_BASE=true
SEGMENT_DURATION=30
MIN_SILENCE_DURATION=1000
```

3. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é:**
```python
# enhanced_transcribe.py
from text_postprocessor import TextPostProcessor
from knowledge_base import KnowledgeBase

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ enhanced_transcribe_audio –≤–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π
```

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–∏–π

–í–º–µ—Å—Ç–æ —Å–ø–ª–æ—à–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ:

### –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Markdown:
```markdown
# –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è: webinar_2024.mp4

**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** 7200 —Å–µ–∫
**–Ø–∑—ã–∫:** ru (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 0.98)
**–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã:** python, gpt, nlp, pytorch, –æ–±—É—á–µ–Ω–∏–µ

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. –í–≤–µ–¥–µ–Ω–∏–µ –∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤...
2. –û–±–∑–æ—Ä —Ç–µ–º –ø—Ä–µ–¥—Å—Ç–æ—è—â–∏—Ö –≤–µ–±–∏–Ω–∞—Ä–æ–≤...
3. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å Python –∏ GPT...

## –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç

### –ê–±–∑–∞—Ü 1
–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç, –≤—Å–µ–º –ø—Ä–∏–≤–µ—Ç. –î–∞–≤–∞–π—Ç–µ –ø–ª—é—Å—ã, –≤–æ–ø—Ä–æ—Å—ã, –æ—Ç–≤–µ—Ç—ã...

### –ê–±–∑–∞—Ü 2
–°–µ–≥–æ–¥–Ω—è –∫–∞–∫ –∫–æ–¥–∏—Ç—å –Ω–∞ Python —Å –ø–æ–º–æ—â—å—é GPT –∏ –±—É–¥—É—Ç –ø—Ä—è–º –ø—Ä–∏–∫–æ–ª—å–Ω—ã–µ —à—Ç—É–∫–∏...
```

### JSON —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:
```json
{
  "metadata": {
    "source_file": "webinar_2024.mp4",
    "duration": 7200.5,
    "language": "ru",
    "processing_time": 45.23
  },
  "key_phrases": ["python", "gpt", "nlp", "pytorch"],
  "structured_content": {
    "paragraphs": ["–¢–µ–∫—Å—Ç –∞–±–∑–∞—Ü–∞ 1...", "–¢–µ–∫—Å—Ç –∞–±–∑–∞—Ü–∞ 2..."],
    "segments": [
      {
        "start": 0.0,
        "end": 5.2,
        "text": "–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç, –≤—Å–µ–º –ø—Ä–∏–≤–µ—Ç...",
        "cleaned_text": "–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç..."
      }
    ]
  }
}
```

## üí° –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞

1. **–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ** - —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∞–±–∑–∞—Ü—ã
2. **–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞** - —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ —Å–ª–æ–≤-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤
3. **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º** - –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
4. **–í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏** - –Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –≤–∏–¥–µ–æ
5. **–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫** - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –≤–µ–±–∏–Ω–∞—Ä–∞–º
6. **–ú—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω–æ—Å—Ç—å** - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ü–µ–ª–µ–π

–í–∞—à–∏ 300+ –≤–µ–±–∏–Ω–∞—Ä–æ–≤ —Ç–µ–ø–µ—Ä—å –ø—Ä–µ–≤—Ä–∞—Ç—è—Ç—Å—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π, –≥–¥–µ –≤—ã —Å–º–æ–∂–µ—Ç–µ –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é! üéâ